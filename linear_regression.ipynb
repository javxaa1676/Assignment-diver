{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6881457f-af7c-47b2-91c0-23ad7ca4954e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5154460-daf0-40ab-ac14-598968606db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchLinearRegression():\n",
    "\n",
    "    def __init__(self, num_iter, lr, no_bias, verbose): \n",
    "        \n",
    "        self.iter = num_iter\n",
    "        self.lr = lr\n",
    "        self.bias = no_bias\n",
    "        self.verbose = verbose\n",
    "     \n",
    "        self.loss = np.zeros(self.iter)\n",
    "        self.val_loss = np.zeros(self.iter)\n",
    "      \n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "\n",
    "        \n",
    "        if self.bias == True:\n",
    "            bias = np.ones((X.shape[0], 1))\n",
    "            X = np.hstack((bias, X))\n",
    "            if X_val is not None:\n",
    "                bias = np.ones((X_val.shape[0], 1))\n",
    "                X_val = np.hstack((bias, X_val))\n",
    "            self.coef_ = np.random.rand(X.shape[1])\n",
    "            self.coef_ = self.coef_.reshape(X.shape[1], 1)\n",
    "    \n",
    "\n",
    "        for epoch in range(self.iter):\n",
    "            y_pred = self._linear_hypothesis(X)\n",
    "            self.loss[epoch] = np.mean((y-y_pred)**2)\n",
    "            \n",
    "            if X_val is not None:\n",
    "                pred_val = self._linear_hypothesis(X_val)\n",
    "                self.val_loss[epoch] = np.mean((y_val-pred_val)**2)\n",
    "                \n",
    "            self.coef_ = self._gradient_descent(X, (y_pred-y))\n",
    "           \n",
    "            if self.verbose == True:\n",
    "                print('{}-th epoch train loss {}'.format(epoch, self.loss[epoch]))\n",
    "                if X_val is not None:\n",
    "                    print('{}-th epoch val loss {}'.format(epoch, self.val_loss[epoch] ))\n",
    "\n",
    "\n",
    "\n",
    "    def _linear_hypothesis(self, X):\n",
    "\n",
    "        pred = X @ self.coef_\n",
    "        \n",
    "        return pred\n",
    "\n",
    "\n",
    "    def _gradient_descent(self, X, error):\n",
    "\n",
    "        for i in range(X.shape[1]):\n",
    "            gradient = error*X[:, i]\n",
    "            self.coef_[i, :] = self.coef_[i, :] - self.lr * np.mean(gradient)\n",
    "\n",
    "        return self.coef_\n",
    "        \n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.bias == True:\n",
    "            bias = np.ones(X.shape[0]).reshape(X.shape[0], 1)\n",
    "            X = np.hstack([bias, X])\n",
    "        pred_y = self._linear_hypothesis(X)\n",
    "        return pred_y\n",
    "\n",
    "\n",
    "    def _mse(self, y_pred, y):\n",
    "\n",
    "        mse = np.mean((y-y_pred)**2)\n",
    "        \n",
    "        return mse\n",
    "\n",
    "\n",
    "    def _loss_func(self, pred, y):\n",
    "\n",
    "        loss = self._mse(pred, y)/2\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4959171e-9d9e-4093-a0ee-1ee1803b9c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "dataset = pd.read_csv(\"import/train.csv\")\n",
    "X = dataset.loc[:, ['GrLivArea', 'YearBuilt']]\n",
    "y = dataset.loc[:, ['SalePrice']]\n",
    "X = X.values\n",
    "X = MinMaxScaler().fit_transform(X)\n",
    "y = np.log(y.values)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c350f88-dc5d-4927-ae4f-8f46ee3bb1ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-th epoch train loss 118.24488788526237\n",
      "0-th epoch val loss 118.50263380290588\n",
      "1-th epoch train loss 114.57301638143988\n",
      "1-th epoch val loss 114.81246393469122\n",
      "2-th epoch train loss 111.01569514100987\n",
      "2-th epoch val loss 111.23756163514287\n",
      "3-th epoch train loss 107.56935928953841\n",
      "3-th epoch val loss 107.77433742682621\n",
      "4-th epoch train loss 104.23055475798884\n",
      "4-th epoch val loss 104.41931343800988\n",
      "5-th epoch train loss 100.99593484072454\n",
      "5-th epoch val loss 101.1691199352522\n",
      "6-th epoch train loss 97.86225686039903\n",
      "6-th epoch val loss 98.0204919636731\n",
      "7-th epoch train loss 94.82637893641433\n",
      "7-th epoch val loss 94.97026609156724\n",
      "8-th epoch train loss 91.88525685373236\n",
      "8-th epoch val loss 92.01537725611911\n",
      "9-th epoch train loss 89.03594102892298\n",
      "9-th epoch val loss 89.15285570708028\n",
      "10-th epoch train loss 86.27557357042994\n",
      "10-th epoch val loss 86.3798240453675\n",
      "11-th epoch train loss 83.60138543012907\n",
      "11-th epoch val loss 83.69349435363395\n",
      "12-th epoch train loss 81.0106936433444\n",
      "12-th epoch val loss 81.09116541595795\n",
      "13-th epoch train loss 78.50089865457534\n",
      "13-th epoch val loss 78.57022002388189\n",
      "14-th epoch train loss 76.0694817262739\n",
      "14-th epoch val loss 76.12812236612042\n",
      "15-th epoch train loss 73.71400242809315\n",
      "15-th epoch val loss 73.76241549933924\n",
      "16-th epoch train loss 71.4320962041082\n",
      "16-th epoch val loss 71.47071889748787\n",
      "17-th epoch train loss 69.22147201558906\n",
      "17-th epoch val loss 69.25072607724664\n",
      "18-th epoch train loss 67.07991005697879\n",
      "18-th epoch val loss 67.10020229722487\n",
      "19-th epoch train loss 65.00525954280485\n",
      "19-th epoch val loss 65.01698232861973\n",
      "20-th epoch train loss 62.995436563320204\n",
      "20-th epoch val loss 62.99896829511725\n",
      "21-th epoch train loss 61.048422006741006\n",
      "21-th epoch val loss 61.04412757988507\n",
      "22-th epoch train loss 59.16225954601219\n",
      "22-th epoch val loss 59.150490797573546\n",
      "23-th epoch train loss 57.335053688098\n",
      "23-th epoch val loss 57.31614982930709\n",
      "24-th epoch train loss 55.56496788385564\n",
      "24-th epoch val loss 55.53925591870901\n",
      "25-th epoch train loss 53.85022269661113\n",
      "25-th epoch val loss 53.81801782706527\n",
      "26-th epoch train loss 52.18909402761445\n",
      "26-th epoch val loss 52.15070004579042\n",
      "27-th epoch train loss 50.57991139660796\n",
      "27-th epoch val loss 50.53562106441636\n",
      "28-th epoch train loss 49.021056275796624\n",
      "28-th epoch val loss 48.97115169237989\n",
      "29-th epoch train loss 47.51096047556204\n",
      "29-th epoch val loss 47.45571343293837\n",
      "30-th epoch train loss 46.04810458031349\n",
      "30-th epoch val loss 45.98777690759484\n",
      "31-th epoch train loss 44.63101643291909\n",
      "31-th epoch val loss 44.565860329464044\n",
      "32-th epoch train loss 43.25826966620877\n",
      "32-th epoch val loss 43.18852802405954\n",
      "33-th epoch train loss 41.92848228008721\n",
      "33-th epoch val loss 41.85438899602949\n",
      "34-th epoch train loss 40.64031526284068\n",
      "34-th epoch val loss 40.56209554041407\n",
      "35-th epoch train loss 39.392471255265335\n",
      "35-th epoch val loss 39.3103418970421\n",
      "36-th epoch train loss 38.18369325628732\n",
      "36-th epoch val loss 38.09786294672709\n",
      "37-th epoch train loss 37.01276336878636\n",
      "37-th epoch val loss 36.92343294796482\n",
      "38-th epoch train loss 35.87850158437425\n",
      "38-th epoch val loss 35.785864312874615\n",
      "39-th epoch train loss 34.7797646059189\n",
      "39-th epoch val loss 34.68400642116567\n",
      "40-th epoch train loss 33.715444706641506\n",
      "40-th epoch val loss 33.61674447094753\n",
      "41-th epoch train loss 32.68446862465144\n",
      "41-th epoch val loss 32.582998365240655\n",
      "42-th epoch train loss 31.685796491818227\n",
      "42-th epoch val loss 31.58172163307826\n",
      "43-th epoch train loss 30.718420795914525\n",
      "43-th epoch val loss 30.611900384125278\n",
      "44-th epoch train loss 29.7813653749968\n",
      "44-th epoch val loss 29.672552295773652\n",
      "45-th epoch train loss 28.873684443022675\n",
      "45-th epoch val loss 28.76272563170516\n",
      "46-th epoch train loss 27.99446164573501\n",
      "46-th epoch val loss 27.881498290944833\n",
      "47-th epoch train loss 27.142809145872697\n",
      "47-th epoch val loss 27.027976886457722\n",
      "48-th epoch train loss 26.317866736797598\n",
      "48-th epoch val loss 26.2012958523719\n",
      "49-th epoch train loss 25.518800983655158\n",
      "49-th epoch val loss 25.400616578938333\n",
      "50-th epoch train loss 24.744804391213705\n",
      "50-th epoch val loss 24.625126574366416\n",
      "51-th epoch train loss 23.995094597553816\n",
      "51-th epoch val loss 23.874038652700474\n",
      "52-th epoch train loss 23.26891359280521\n",
      "52-th epoch val loss 23.146590146928432\n",
      "53-th epoch train loss 22.565526962153196\n",
      "53-th epoch val loss 22.442042146539116\n",
      "54-th epoch train loss 21.88422315236102\n",
      "54-th epoch val loss 21.75967875876876\n",
      "55-th epoch train loss 21.224312761077872\n",
      "55-th epoch val loss 21.09880639280113\n",
      "56-th epoch train loss 20.585127848224943\n",
      "56-th epoch val loss 20.458753066208267\n",
      "57-th epoch train loss 19.966021268773833\n",
      "57-th epoch val loss 19.83886773294119\n",
      "58-th epoch train loss 19.3663660262531\n",
      "58-th epoch val loss 19.238519632201214\n",
      "59-th epoch train loss 18.78555464633909\n",
      "59-th epoch val loss 18.657097657543414\n",
      "60-th epoch train loss 18.22299856990744\n",
      "60-th epoch val loss 18.094009745583783\n",
      "61-th epoch train loss 17.678127564940777\n",
      "61-th epoch val loss 17.548682283701332\n",
      "62-th epoch train loss 17.150389156707167\n",
      "62-th epoch val loss 17.020559536145075\n",
      "63-th epoch train loss 16.63924807564176\n",
      "63-th epoch val loss 16.5091030879743\n",
      "64-th epoch train loss 16.144185722381863\n",
      "64-th epoch val loss 16.013791306278225\n",
      "65-th epoch train loss 15.66469964942282\n",
      "65-th epoch val loss 15.534118818138413\n",
      "66-th epoch train loss 15.20030305887839\n",
      "66-th epoch val loss 15.069596004813787\n",
      "67-th epoch train loss 14.750524315845546\n",
      "67-th epoch val loss 14.619748511644527\n",
      "68-th epoch train loss 14.31490647688902\n",
      "68-th epoch val loss 14.184116773186492\n",
      "69-th epoch train loss 13.893006833176052\n",
      "69-th epoch val loss 13.762255553103142\n",
      "70-th epoch train loss 13.484396467806356\n",
      "70-th epoch val loss 13.353733498356663\n",
      "71-th epoch train loss 13.088659826896375\n",
      "71-th epoch val loss 12.958132707253949\n",
      "72-th epoch train loss 12.705394303990731\n",
      "72-th epoch val loss 12.575048310917339\n",
      "73-th epoch train loss 12.334209837386869\n",
      "73-th epoch val loss 12.204088067762882\n",
      "74-th epoch train loss 11.974728519971903\n",
      "74-th epoch val loss 11.84487197058228\n",
      "75-th epoch train loss 11.626584221183023\n",
      "75-th epoch val loss 11.497031865836885\n",
      "76-th epoch train loss 11.28942222071491\n",
      "76-th epoch val loss 11.160211084784466\n",
      "77-th epoch train loss 10.96289885360932\n",
      "77-th epoch val loss 10.834064086071189\n",
      "78-th epoch train loss 10.646681166373375\n",
      "78-th epoch val loss 10.518256109432679\n",
      "79-th epoch train loss 10.340446583783875\n",
      "79-th epoch val loss 10.212462840159006\n",
      "80-th epoch train loss 10.043882586045896\n",
      "80-th epoch val loss 9.916370083989321\n",
      "81-th epoch train loss 9.756686395983943\n",
      "81-th epoch val loss 9.629673452112135\n",
      "82-th epoch train loss 9.478564675954102\n",
      "82-th epoch val loss 9.352078055957236\n",
      "83-th epoch train loss 9.20923323417525\n",
      "83-th epoch val loss 9.083298211475185\n",
      "84-th epoch train loss 8.94841674018675\n",
      "84-th epoch val loss 8.823057152609595\n",
      "85-th epoch train loss 8.695848449149171\n",
      "85-th epoch val loss 8.571086753676626\n",
      "86-th epoch train loss 8.451269934713318\n",
      "86-th epoch val loss 8.32712726037499\n",
      "87-th epoch train loss 8.214430830191487\n",
      "87-th epoch val loss 8.090927029158362\n",
      "88-th epoch train loss 7.985088577773003\n",
      "88-th epoch val loss 7.862242274710399\n",
      "89-th epoch train loss 7.763008185534235\n",
      "89-th epoch val loss 7.640836825270654\n",
      "90-th epoch train loss 7.547961992000918\n",
      "90-th epoch val loss 7.426481885567471\n",
      "91-th epoch train loss 7.339729438028183\n",
      "91-th epoch val loss 7.218955807121515\n",
      "92-th epoch train loss 7.138096845771063\n",
      "92-th epoch val loss 7.018043865691017\n",
      "93-th epoch train loss 6.942857204525142\n",
      "93-th epoch val loss 6.8235380456367825\n",
      "94-th epoch train loss 6.753809963223947\n",
      "94-th epoch val loss 6.635236830991973\n",
      "95-th epoch train loss 6.570760829386361\n",
      "95-th epoch val loss 6.452945003028457\n",
      "96-th epoch train loss 6.393521574313614\n",
      "96-th epoch val loss 6.276473444117735\n",
      "97-th epoch train loss 6.221909844341756\n",
      "97-th epoch val loss 6.105638947690978\n",
      "98-th epoch train loss 6.055748977961502\n",
      "98-th epoch val loss 5.940264034108657\n",
      "99-th epoch train loss 5.894867828623166\n",
      "99-th epoch val loss 5.780176772256113\n",
      "100-th epoch train loss 5.7391005930500665\n",
      "100-th epoch val loss 5.625210606687208\n",
      "101-th epoch train loss 5.588286644889293\n",
      "101-th epoch val loss 5.475204190143614\n",
      "102-th epoch train loss 5.442270373534036\n",
      "102-th epoch val loss 5.330001221282787\n",
      "103-th epoch train loss 5.300901027956777\n",
      "103-th epoch val loss 5.189450287452694\n",
      "104-th epoch train loss 5.164032565397704\n",
      "104-th epoch val loss 5.053404712356536\n",
      "105-th epoch train loss 5.031523504757512\n",
      "105-th epoch val loss 4.921722408455519\n",
      "106-th epoch train loss 4.9032367845484\n",
      "106-th epoch val loss 4.794265733962389\n",
      "107-th epoch train loss 4.7790396252616985\n",
      "107-th epoch val loss 4.670901354283154\n",
      "108-th epoch train loss 4.6588033960148865\n",
      "108-th epoch val loss 4.551500107768722\n",
      "109-th epoch train loss 4.542403485345027\n",
      "109-th epoch val loss 4.435936875642513\n",
      "110-th epoch train loss 4.429719176019832\n",
      "110-th epoch val loss 4.324090455974328\n",
      "111-th epoch train loss 4.320633523741508\n",
      "111-th epoch val loss 4.2158434415746715\n",
      "112-th epoch train loss 4.215033239622434\n",
      "112-th epoch val loss 4.11108210168774\n",
      "113-th epoch train loss 4.112808576315498\n",
      "113-th epoch val loss 4.009696267365014\n",
      "114-th epoch train loss 4.013853217685499\n",
      "114-th epoch val loss 3.9115792204050326\n",
      "115-th epoch train loss 3.918064171911665\n",
      "115-th epoch val loss 3.8166275857485994\n",
      "116-th epoch train loss 3.8253416679145693\n",
      "116-th epoch val loss 3.7247412272219353\n",
      "117-th epoch train loss 3.7355890550042647\n",
      "117-th epoch val loss 3.6358231465238107\n",
      "118-th epoch train loss 3.64871270564945\n",
      "118-th epoch val loss 3.5497793853557686\n",
      "119-th epoch train loss 3.564621921270756\n",
      "119-th epoch val loss 3.4665189305978115\n",
      "120-th epoch train loss 3.483228840964139\n",
      "120-th epoch val loss 3.385953622434841\n",
      "121-th epoch train loss 3.4044483530633434\n",
      "121-th epoch val loss 3.3079980653421632\n",
      "122-th epoch train loss 3.328198009453237\n",
      "122-th epoch val loss 3.2325695418412113\n",
      "123-th epoch train loss 3.2543979425484966\n",
      "123-th epoch val loss 3.1595879289393403\n",
      "124-th epoch train loss 3.182970784854842\n",
      "124-th epoch val loss 3.0889756171702927\n",
      "125-th epoch train loss 3.1138415910325596\n",
      "125-th epoch val loss 3.020657432154489\n",
      "126-th epoch train loss 3.0469377623845495\n",
      "126-th epoch val loss 2.95456055860081\n",
      "127-th epoch train loss 2.982188973693551\n",
      "127-th epoch val loss 2.890614466673988\n",
      "128-th epoch train loss 2.9195271023355565\n",
      "128-th epoch val loss 2.828750840654059\n",
      "129-th epoch train loss 2.858886159598634\n",
      "129-th epoch val loss 2.768903509816628\n",
      "130-th epoch train loss 2.8002022241386704\n",
      "130-th epoch val loss 2.711008381464908\n",
      "131-th epoch train loss 2.7434133775055867\n",
      "131-th epoch val loss 2.6550033760466563\n",
      "132-th epoch train loss 2.6884596416756747\n",
      "132-th epoch val loss 2.600828364291137\n",
      "133-th epoch train loss 2.6352829185277393\n",
      "133-th epoch val loss 2.548425106303395\n",
      "134-th epoch train loss 2.5838269312025903\n",
      "134-th epoch val loss 2.497737192554904\n",
      "135-th epoch train loss 2.5340371672873663\n",
      "135-th epoch val loss 2.448709986711675\n",
      "136-th epoch train loss 2.485860823767988\n",
      "136-th epoch val loss 2.401290570242704\n",
      "137-th epoch train loss 2.439246753694741\n",
      "137-th epoch val loss 2.355427688753359\n",
      "138-th epoch train loss 2.3941454145077885\n",
      "138-th epoch val loss 2.311071699990121\n",
      "139-th epoch train loss 2.3505088179709794\n",
      "139-th epoch val loss 2.2681745234646877\n",
      "140-th epoch train loss 2.308290481663971\n",
      "140-th epoch val loss 2.226689591647076\n",
      "141-th epoch train loss 2.267445381984239\n",
      "141-th epoch val loss 2.186571802678965\n",
      "142-th epoch train loss 2.227929908612025\n",
      "142-th epoch val loss 2.1477774745599785\n",
      "143-th epoch train loss 2.189701820392737\n",
      "143-th epoch val loss 2.110264300761114\n",
      "144-th epoch train loss 2.152720202592773\n",
      "144-th epoch val loss 2.0739913072209446\n",
      "145-th epoch train loss 2.1169454254860245\n",
      "145-th epoch val loss 2.0389188106815697\n",
      "146-th epoch train loss 2.0823391042297277\n",
      "146-th epoch val loss 2.005008378322671\n",
      "147-th epoch train loss 2.048864059989567\n",
      "147-th epoch val loss 1.9722227886533006\n",
      "148-th epoch train loss 2.016484282275186\n",
      "148-th epoch val loss 1.9405259936222565\n",
      "149-th epoch train loss 1.9851648924484808\n",
      "149-th epoch val loss 1.9098830819091894\n",
      "150-th epoch train loss 1.954872108368209\n",
      "150-th epoch val loss 1.8802602433596727\n",
      "151-th epoch train loss 1.9255732101355916\n",
      "151-th epoch val loss 1.8516247345286867\n",
      "152-th epoch train loss 1.8972365069066597\n",
      "152-th epoch val loss 1.823944845298017\n",
      "153-th epoch train loss 1.8698313047381976\n",
      "153-th epoch val loss 1.79718986653418\n",
      "154-th epoch train loss 1.843327875435123\n",
      "154-th epoch val loss 1.7713300587544867\n",
      "155-th epoch train loss 1.8176974263681864\n",
      "155-th epoch val loss 1.746336621769914\n",
      "156-th epoch train loss 1.7929120712317932\n",
      "156-th epoch val loss 1.7221816652743587\n",
      "157-th epoch train loss 1.7689448017127332\n",
      "157-th epoch val loss 1.698838180350863\n",
      "158-th epoch train loss 1.7457694600414841\n",
      "158-th epoch val loss 1.6762800118662722\n",
      "159-th epoch train loss 1.7233607123986374\n",
      "159-th epoch val loss 1.6544818317266763\n",
      "160-th epoch train loss 1.7016940231498594\n",
      "160-th epoch val loss 1.633419112966864\n",
      "161-th epoch train loss 1.6807456298836216\n",
      "161-th epoch val loss 1.6130681046478401\n",
      "162-th epoch train loss 1.6604925192267244\n",
      "162-th epoch val loss 1.5934058075372504\n",
      "163-th epoch train loss 1.6409124034134333\n",
      "163-th epoch val loss 1.5744099505483675\n",
      "164-th epoch train loss 1.6219836975847832\n",
      "164-th epoch val loss 1.556058967914024\n",
      "165-th epoch train loss 1.6036854977953356\n",
      "165-th epoch val loss 1.5383319770726214\n",
      "166-th epoch train loss 1.5859975597054021\n",
      "166-th epoch val loss 1.5212087572440656\n",
      "167-th epoch train loss 1.5689002779373848\n",
      "167-th epoch val loss 1.5046697286741433\n",
      "168-th epoch train loss 1.5523746660755944\n",
      "168-th epoch val loss 1.4886959325265408\n",
      "169-th epoch train loss 1.5364023372895315\n",
      "169-th epoch val loss 1.473269011402357\n",
      "170-th epoch train loss 1.520965485561216\n",
      "170-th epoch val loss 1.4583711904675651\n",
      "171-th epoch train loss 1.506046867497795\n",
      "171-th epoch val loss 1.4439852591695077\n",
      "172-th epoch train loss 1.4916297847112079\n",
      "172-th epoch val loss 1.4300945535240925\n",
      "173-th epoch train loss 1.4776980667472728\n",
      "173-th epoch val loss 1.4166829389559166\n",
      "174-th epoch train loss 1.4642360545470958\n",
      "174-th epoch val loss 1.4037347936741116\n",
      "175-th epoch train loss 1.4512285844242403\n",
      "175-th epoch val loss 1.3912349925672258\n",
      "176-th epoch train loss 1.4386609725416128\n",
      "176-th epoch val loss 1.379168891600991\n",
      "177-th epoch train loss 1.426518999872517\n",
      "177-th epoch val loss 1.3675223127033143\n",
      "178-th epoch train loss 1.4147888976308005\n",
      "178-th epoch val loss 1.35628152912132\n",
      "179-th epoch train loss 1.4034573331555082\n",
      "179-th epoch val loss 1.3454332512357512\n",
      "180-th epoch train loss 1.3925113962358893\n",
      "180-th epoch val loss 1.334964612818481\n",
      "181-th epoch train loss 1.3819385858630617\n",
      "181-th epoch val loss 1.3248631577193426\n",
      "182-th epoch train loss 1.3717267973950482\n",
      "182-th epoch val loss 1.3151168269689013\n",
      "183-th epoch train loss 1.361864310122324\n",
      "183-th epoch val loss 1.3057139462842184\n",
      "184-th epoch train loss 1.3523397752214086\n",
      "184-th epoch val loss 1.296643213965058\n",
      "185-th epoch train loss 1.3431422040844208\n",
      "185-th epoch val loss 1.2878936891683654\n",
      "186-th epoch train loss 1.3342609570128952\n",
      "186-th epoch val loss 1.279454780549247\n",
      "187-th epoch train loss 1.3256857322645306\n",
      "187-th epoch val loss 1.2713162352570293\n",
      "188-th epoch train loss 1.317406555441866\n",
      "188-th epoch val loss 1.2634681282753335\n",
      "189-th epoch train loss 1.309413769212257\n",
      "189-th epoch val loss 1.2559008520954513\n",
      "190-th epoch train loss 1.3016980233488256\n",
      "190-th epoch val loss 1.24860510671264\n",
      "191-th epoch train loss 1.2942502650823975\n",
      "191-th epoch val loss 1.2415718899352675\n",
      "192-th epoch train loss 1.2870617297547422\n",
      "192-th epoch val loss 1.2347924879970695\n",
      "193-th epoch train loss 1.2801239317637307\n",
      "193-th epoch val loss 1.2282584664630614\n",
      "194-th epoch train loss 1.2734286557913295\n",
      "194-th epoch val loss 1.2219616614199713\n",
      "195-th epoch train loss 1.266967948305617\n",
      "195-th epoch val loss 1.21589417094231\n",
      "196-th epoch train loss 1.2607341093282838\n",
      "196-th epoch val loss 1.210048346825489\n",
      "197-th epoch train loss 1.2547196844593655\n",
      "197-th epoch val loss 1.2044167865776827\n",
      "198-th epoch train loss 1.2489174571511763\n",
      "198-th epoch val loss 1.1989923256623385\n",
      "199-th epoch train loss 1.2433204412237\n",
      "199-th epoch val loss 1.193768029983551\n",
      "200-th epoch train loss 1.237921873613904\n",
      "200-th epoch val loss 1.1887371886067062\n",
      "201-th epoch train loss 1.232715207351696\n",
      "201-th epoch val loss 1.1838933067070783\n",
      "202-th epoch train loss 1.2276941047554641\n",
      "202-th epoch val loss 1.1792300987392577\n",
      "203-th epoch train loss 1.2228524308403574\n",
      "203-th epoch val loss 1.1747414818205348\n",
      "204-th epoch train loss 1.2181842469326762\n",
      "204-th epoch val loss 1.1704215693215614\n",
      "205-th epoch train loss 1.2136838044839604\n",
      "205-th epoch val loss 1.166264664657831\n",
      "206-th epoch train loss 1.2093455390785386\n",
      "206-th epoch val loss 1.1622652552757073\n",
      "207-th epoch train loss 1.2051640646285258\n",
      "207-th epoch val loss 1.1584180068269416\n",
      "208-th epoch train loss 1.2011341677504177\n",
      "208-th epoch val loss 1.1547177575257945\n",
      "209-th epoch train loss 1.1972508023176227\n",
      "209-th epoch val loss 1.151159512683064\n",
      "210-th epoch train loss 1.1935090841834586\n",
      "210-th epoch val loss 1.1477384394115093\n",
      "211-th epoch train loss 1.1899042860692848\n",
      "211-th epoch val loss 1.1444498614973113\n",
      "212-th epoch train loss 1.1864318326126377\n",
      "212-th epoch val loss 1.1412892544323991\n",
      "213-th epoch train loss 1.183087295570374\n",
      "213-th epoch val loss 1.1382522406026157\n",
      "214-th epoch train loss 1.1798663891719912\n",
      "214-th epoch val loss 1.1353345846268674\n",
      "215-th epoch train loss 1.176764965618444\n",
      "215-th epoch val loss 1.1325321888425337\n",
      "216-th epoch train loss 1.1737790107219193\n",
      "216-th epoch val loss 1.129841088932583\n",
      "217-th epoch train loss 1.170904639682176\n",
      "217-th epoch val loss 1.1272574496899652\n",
      "218-th epoch train loss 1.168138092995191\n",
      "218-th epoch val loss 1.1247775609149973\n",
      "219-th epoch train loss 1.1654757324899834\n",
      "219-th epoch val loss 1.1223978334415867\n",
      "220-th epoch train loss 1.162914037489624\n",
      "220-th epoch val loss 1.120114795288279\n",
      "221-th epoch train loss 1.1604496010925527\n",
      "221-th epoch val loss 1.1179250879302183\n",
      "222-th epoch train loss 1.1580791265704509\n",
      "222-th epoch val loss 1.1158254626882549\n",
      "223-th epoch train loss 1.155799423879038\n",
      "223-th epoch val loss 1.113812777231537\n",
      "224-th epoch train loss 1.1536074062782649\n",
      "224-th epoch val loss 1.1118839921900463\n",
      "225-th epoch train loss 1.151500087058491\n",
      "225-th epoch val loss 1.1100361678736326\n",
      "226-th epoch train loss 1.1494745763693432\n",
      "226-th epoch val loss 1.1082664610942339\n",
      "227-th epoch train loss 1.1475280781480461\n",
      "227-th epoch val loss 1.1065721220880462\n",
      "228-th epoch train loss 1.1456578871441234\n",
      "228-th epoch val loss 1.1049504915345285\n",
      "229-th epoch train loss 1.143861386037463\n",
      "229-th epoch val loss 1.1033989976692087\n",
      "230-th epoch train loss 1.142136042646831\n",
      "230-th epoch val loss 1.101915153487369\n",
      "231-th epoch train loss 1.140479407226008\n",
      "231-th epoch val loss 1.1004965540357565\n",
      "232-th epoch train loss 1.1388891098448215\n",
      "232-th epoch val loss 1.0991408737895818\n",
      "233-th epoch train loss 1.13736285785241\n",
      "233-th epoch val loss 1.0978458641121258\n",
      "234-th epoch train loss 1.1358984334201672\n",
      "234-th epoch val loss 1.0966093507943846\n",
      "235-th epoch train loss 1.1344936911618664\n",
      "235-th epoch val loss 1.095429231672236\n",
      "236-th epoch train loss 1.1331465558285592\n",
      "236-th epoch val loss 1.0943034743187137\n",
      "237-th epoch train loss 1.1318550200759154\n",
      "237-th epoch val loss 1.0932301138090352\n",
      "238-th epoch train loss 1.130617142301735\n",
      "238-th epoch val loss 1.0922072505561093\n",
      "239-th epoch train loss 1.1294310445514493\n",
      "239-th epoch val loss 1.0912330482143138\n",
      "240-th epoch train loss 1.1282949104894793\n",
      "240-th epoch val loss 1.0903057316494136\n",
      "241-th epoch train loss 1.1272069834343958\n",
      "241-th epoch val loss 1.089423584972544\n",
      "242-th epoch train loss 1.1261655644558934\n",
      "242-th epoch val loss 1.088584949636258\n",
      "243-th epoch train loss 1.1251690105316352\n",
      "243-th epoch val loss 1.0877882225906899\n",
      "244-th epoch train loss 1.124215732762111\n",
      "244-th epoch val loss 1.0870318544979585\n",
      "245-th epoch train loss 1.1233041946416806\n",
      "245-th epoch val loss 1.0863143480029829\n",
      "246-th epoch train loss 1.1224329103840633\n",
      "246-th epoch val loss 1.085634256058946\n",
      "247-th epoch train loss 1.121600443300553\n",
      "247-th epoch val loss 1.0849901803056923\n",
      "248-th epoch train loss 1.120805404229323\n",
      "248-th epoch val loss 1.0843807694994003\n",
      "249-th epoch train loss 1.120046450014221\n",
      "249-th epoch val loss 1.0838047179919343\n",
      "250-th epoch train loss 1.119322282031502\n",
      "250-th epoch val loss 1.0832607642582983\n",
      "251-th epoch train loss 1.1186316447630025\n",
      "251-th epoch val loss 1.0827476894707067\n",
      "252-th epoch train loss 1.1179733244143042\n",
      "252-th epoch val loss 1.082264316117794\n",
      "253-th epoch train loss 1.1173461475764788\n",
      "253-th epoch val loss 1.0818095066675586\n",
      "254-th epoch train loss 1.1167489799300494\n",
      "254-th epoch val loss 1.0813821622726636\n",
      "255-th epoch train loss 1.1161807249898463\n",
      "255-th epoch val loss 1.0809812215167678\n",
      "256-th epoch train loss 1.1156403228894847\n",
      "256-th epoch val loss 1.0806056592006057\n",
      "257-th epoch train loss 1.1151267492042125\n",
      "257-th epoch val loss 1.080254485166558\n",
      "258-th epoch train loss 1.1146390138109352\n",
      "258-th epoch val loss 1.0799267431605157\n",
      "259-th epoch train loss 1.1141761597842528\n",
      "259-th epoch val loss 1.0796215097298634\n",
      "260-th epoch train loss 1.1137372623273791\n",
      "260-th epoch val loss 1.079337893156446\n",
      "261-th epoch train loss 1.113321427736851\n",
      "261-th epoch val loss 1.0790750324234262\n",
      "262-th epoch train loss 1.1129277923999716\n",
      "262-th epoch val loss 1.0788320962149633\n",
      "263-th epoch train loss 1.1125555218239587\n",
      "263-th epoch val loss 1.0786082819476843\n",
      "264-th epoch train loss 1.1122038096958087\n",
      "264-th epoch val loss 1.0784028148329514\n",
      "265-th epoch train loss 1.1118718769719083\n",
      "265-th epoch val loss 1.0782149469689564\n",
      "266-th epoch train loss 1.111558970996469\n",
      "266-th epoch val loss 1.078043956461702\n",
      "267-th epoch train loss 1.111264364647875\n",
      "267-th epoch val loss 1.0778891465739684\n",
      "268-th epoch train loss 1.1109873555120682\n",
      "268-th epoch val loss 1.0777498449013814\n",
      "269-th epoch train loss 1.1107272650821287\n",
      "269-th epoch val loss 1.0776254025747283\n",
      "270-th epoch train loss 1.1104834379832296\n",
      "270-th epoch val loss 1.077515193487704\n",
      "271-th epoch train loss 1.110255241222156\n",
      "271-th epoch val loss 1.0774186135492763\n",
      "272-th epoch train loss 1.1100420634606405\n",
      "272-th epoch val loss 1.0773350799599055\n",
      "273-th epoch train loss 1.1098433143117459\n",
      "273-th epoch val loss 1.0772640305108656\n",
      "274-th epoch train loss 1.1096584236585882\n",
      "274-th epoch val loss 1.0772049229059328\n",
      "275-th epoch train loss 1.109486840994688\n",
      "275-th epoch val loss 1.0771572341047504\n",
      "276-th epoch train loss 1.109328034785278\n",
      "276-th epoch val loss 1.0771204596871737\n",
      "277-th epoch train loss 1.1091814918489045\n",
      "277-th epoch val loss 1.0770941132379448\n",
      "278-th epoch train loss 1.1090467167586853\n",
      "278-th epoch val loss 1.07707772575105\n",
      "279-th epoch train loss 1.1089232312626145\n",
      "279-th epoch val loss 1.0770708450531463\n",
      "280-th epoch train loss 1.1088105737223035\n",
      "280-th epoch val loss 1.07707303524545\n",
      "281-th epoch train loss 1.1087082985695866\n",
      "281-th epoch val loss 1.0770838761635033\n",
      "282-th epoch train loss 1.1086159757804295\n",
      "282-th epoch val loss 1.0771029628542688\n",
      "283-th epoch train loss 1.1085331903655944\n",
      "283-th epoch val loss 1.077129905069988\n",
      "284-th epoch train loss 1.1084595418775374\n",
      "284-th epoch val loss 1.0771643267782869\n",
      "285-th epoch train loss 1.1083946439330237\n",
      "285-th epoch val loss 1.077205865688012\n",
      "286-th epoch train loss 1.108338123750971\n",
      "286-th epoch val loss 1.077254172790298\n",
      "287-th epoch train loss 1.1082896217050384\n",
      "287-th epoch val loss 1.077308911914393\n",
      "288-th epoch train loss 1.108248790890498\n",
      "288-th epoch val loss 1.0773697592977647\n",
      "289-th epoch train loss 1.1082152967049421\n",
      "289-th epoch val loss 1.0774364031700503\n",
      "290-th epoch train loss 1.1081888164423852\n",
      "290-th epoch val loss 1.077508543350398\n",
      "291-th epoch train loss 1.1081690389003434\n",
      "291-th epoch val loss 1.0775858908577869\n",
      "292-th epoch train loss 1.10815566399948\n",
      "292-th epoch val loss 1.0776681675339124\n",
      "293-th epoch train loss 1.108148402415425\n",
      "293-th epoch val loss 1.0777551056782375\n",
      "294-th epoch train loss 1.1081469752223778\n",
      "294-th epoch val loss 1.0778464476948286\n",
      "295-th epoch train loss 1.1081511135481317\n",
      "295-th epoch val loss 1.0779419457505979\n",
      "296-th epoch train loss 1.108160558240149\n",
      "296-th epoch val loss 1.0780413614445994\n",
      "297-th epoch train loss 1.108175059542348\n",
      "297-th epoch val loss 1.078144465488018\n",
      "298-th epoch train loss 1.1081943767822582\n",
      "298-th epoch val loss 1.078251037394523\n",
      "299-th epoch train loss 1.1082182780682177\n",
      "299-th epoch val loss 1.07836086518065\n",
      "300-th epoch train loss 1.1082465399962973\n",
      "300-th epoch val loss 1.0784737450758983\n",
      "301-th epoch train loss 1.1082789473666437\n",
      "301-th epoch val loss 1.0785894812422339\n",
      "302-th epoch train loss 1.108315292908943\n",
      "302-th epoch val loss 1.0787078855026973\n",
      "303-th epoch train loss 1.1083553770167163\n",
      "303-th epoch val loss 1.0788287770788332\n",
      "304-th epoch train loss 1.1083990074901737\n",
      "304-th epoch val loss 1.078951982336656\n",
      "305-th epoch train loss 1.108445999287348\n",
      "305-th epoch val loss 1.079077334540883\n",
      "306-th epoch train loss 1.1084961742832504\n",
      "306-th epoch val loss 1.0792046736171734\n",
      "307-th epoch train loss 1.1085493610367985\n",
      "307-th epoch val loss 1.0793338459221156\n",
      "308-th epoch train loss 1.1086053945652619\n",
      "308-th epoch val loss 1.0794647040207213\n",
      "309-th epoch train loss 1.108664116125996\n",
      "309-th epoch val loss 1.0795971064711791\n",
      "310-th epoch train loss 1.1087253730052284\n",
      "310-th epoch val loss 1.0797309176166532\n",
      "311-th epoch train loss 1.1087890183136742\n",
      "311-th epoch val loss 1.0798660073838777\n",
      "312-th epoch train loss 1.1088549107887697\n",
      "312-th epoch val loss 1.0800022510883565\n",
      "313-th epoch train loss 1.1089229146033066\n",
      "313-th epoch val loss 1.0801395292459413\n",
      "314-th epoch train loss 1.1089928991802722\n",
      "314-th epoch val loss 1.0802777273905897\n",
      "315-th epoch train loss 1.109064739013689\n",
      "315-th epoch val loss 1.080416735898107\n",
      "316-th epoch train loss 1.1091383134952761\n",
      "316-th epoch val loss 1.0805564498156797\n",
      "317-th epoch train loss 1.1092135067467346\n",
      "317-th epoch val loss 1.080696768697017\n",
      "318-th epoch train loss 1.109290207457489\n",
      "318-th epoch val loss 1.0808375964429207\n",
      "319-th epoch train loss 1.109368308727704\n",
      "319-th epoch val loss 1.080978841147107\n",
      "320-th epoch train loss 1.109447707916414\n",
      "320-th epoch val loss 1.0811204149471207\n",
      "321-th epoch train loss 1.1095283064946013\n",
      "321-th epoch val loss 1.0812622338801698\n",
      "322-th epoch train loss 1.1096100099030632\n",
      "322-th epoch val loss 1.081404217743726\n",
      "323-th epoch train loss 1.1096927274149209\n",
      "323-th epoch val loss 1.0815462899607449\n",
      "324-th epoch train loss 1.1097763720026177\n",
      "324-th epoch val loss 1.0816883774493462\n",
      "325-th epoch train loss 1.109860860209266\n",
      "325-th epoch val loss 1.081830410496823\n",
      "326-th epoch train loss 1.1099461120242053\n",
      "326-th epoch val loss 1.081972322637833\n",
      "327-th epoch train loss 1.1100320507626342\n",
      "327-th epoch val loss 1.0821140505366382\n",
      "328-th epoch train loss 1.110118602949191\n",
      "328-th epoch val loss 1.0822555338732747\n",
      "329-th epoch train loss 1.1102056982053508\n",
      "329-th epoch val loss 1.0823967152335034\n",
      "330-th epoch train loss 1.1102932691405256\n",
      "330-th epoch val loss 1.0825375400024475\n",
      "331-th epoch train loss 1.110381251246742\n",
      "331-th epoch val loss 1.082677956261774\n",
      "332-th epoch train loss 1.1104695827967843\n",
      "332-th epoch val loss 1.0828179146903258\n",
      "333-th epoch train loss 1.1105582047456966\n",
      "333-th epoch val loss 1.0829573684680733\n",
      "334-th epoch train loss 1.1106470606355283\n",
      "334-th epoch val loss 1.0830962731832967\n",
      "335-th epoch train loss 1.11073609650323\n",
      "335-th epoch val loss 1.0832345867428859\n",
      "336-th epoch train loss 1.110825260791587\n",
      "336-th epoch val loss 1.0833722692856536\n",
      "337-th epoch train loss 1.1109145042631046\n",
      "337-th epoch val loss 1.0835092830985769\n",
      "338-th epoch train loss 1.1110037799167434\n",
      "338-th epoch val loss 1.0836455925358617\n",
      "339-th epoch train loss 1.1110930429074144\n",
      "339-th epoch val loss 1.0837811639407398\n",
      "340-th epoch train loss 1.111182250468148\n",
      "340-th epoch val loss 1.083915965569919\n",
      "341-th epoch train loss 1.1112713618348504\n",
      "341-th epoch val loss 1.084049967520592\n",
      "342-th epoch train loss 1.1113603381735628\n",
      "342-th epoch val loss 1.0841831416599217\n",
      "343-th epoch train loss 1.111449142510142\n",
      "343-th epoch val loss 1.0843154615569275\n",
      "344-th epoch train loss 1.1115377396622923\n",
      "344-th epoch val loss 1.0844469024166925\n",
      "345-th epoch train loss 1.1116260961738607\n",
      "345-th epoch val loss 1.0845774410168132\n",
      "346-th epoch train loss 1.1117141802513342\n",
      "346-th epoch val loss 1.084707055646024\n",
      "347-th epoch train loss 1.111801961702462\n",
      "347-th epoch val loss 1.0848357260449224\n",
      "348-th epoch train loss 1.1118894118769367\n",
      "348-th epoch val loss 1.0849634333487297\n",
      "349-th epoch train loss 1.111976503609065\n",
      "349-th epoch val loss 1.0850901600320146\n",
      "350-th epoch train loss 1.1120632111623692\n",
      "350-th epoch val loss 1.0852158898553261\n",
      "351-th epoch train loss 1.1121495101760521\n",
      "351-th epoch val loss 1.0853406078136614\n",
      "352-th epoch train loss 1.1122353776132687\n",
      "352-th epoch val loss 1.085464300086721\n",
      "353-th epoch train loss 1.1123207917111433\n",
      "353-th epoch val loss 1.0855869539908842\n",
      "354-th epoch train loss 1.1124057319324774\n",
      "354-th epoch val loss 1.0857085579328478\n",
      "355-th epoch train loss 1.1124901789190962\n",
      "355-th epoch val loss 1.0858291013648866\n",
      "356-th epoch train loss 1.1125741144467742\n",
      "356-th epoch val loss 1.0859485747416606\n",
      "357-th epoch train loss 1.1126575213816947\n",
      "357-th epoch val loss 1.0860669694785434\n",
      "358-th epoch train loss 1.1127403836383922\n",
      "358-th epoch val loss 1.086184277911402\n",
      "359-th epoch train loss 1.1128226861391266\n",
      "359-th epoch val loss 1.0863004932577878\n",
      "360-th epoch train loss 1.1129044147746465\n",
      "360-th epoch val loss 1.0864156095794968\n",
      "361-th epoch train loss 1.1129855563662954\n",
      "361-th epoch val loss 1.0865296217464464\n",
      "362-th epoch train loss 1.1130660986294145\n",
      "362-th epoch val loss 1.0866425254018288\n",
      "363-th epoch train loss 1.1131460301380058\n",
      "363-th epoch val loss 1.0867543169284994\n",
      "364-th epoch train loss 1.1132253402906067\n",
      "364-th epoch val loss 1.0868649934165604\n",
      "365-th epoch train loss 1.1133040192773442\n",
      "365-th epoch val loss 1.0869745526320957\n",
      "366-th epoch train loss 1.1133820580481255\n",
      "366-th epoch val loss 1.0870829929870234\n",
      "367-th epoch train loss 1.1134594482819287\n",
      "367-th epoch val loss 1.0871903135100298\n",
      "368-th epoch train loss 1.113536182357159\n",
      "368-th epoch val loss 1.087296513818544\n",
      "369-th epoch train loss 1.1136122533230355\n",
      "369-th epoch val loss 1.0874015940917183\n",
      "370-th epoch train loss 1.113687654871972\n",
      "370-th epoch val loss 1.0875055550443935\n",
      "371-th epoch train loss 1.1137623813129252\n",
      "371-th epoch val loss 1.087608397902\n",
      "372-th epoch train loss 1.113836427545671\n",
      "372-th epoch val loss 1.0877101243763685\n",
      "373-th epoch train loss 1.1139097890359826\n",
      "373-th epoch val loss 1.0878107366424294\n",
      "374-th epoch train loss 1.1139824617916871\n",
      "374-th epoch val loss 1.08791023731576\n",
      "375-th epoch train loss 1.114054442339553\n",
      "375-th epoch val loss 1.0880086294309512\n",
      "376-th epoch train loss 1.1141257277030059\n",
      "376-th epoch val loss 1.0881059164207783\n",
      "377-th epoch train loss 1.1141963153806251\n",
      "377-th epoch val loss 1.0882021020961345\n",
      "378-th epoch train loss 1.1142662033254045\n",
      "378-th epoch val loss 1.0882971906267098\n",
      "379-th epoch train loss 1.1143353899247528\n",
      "379-th epoch val loss 1.08839118652239\n",
      "380-th epoch train loss 1.1144038739812046\n",
      "380-th epoch val loss 1.088484094615348\n",
      "381-th epoch train loss 1.1144716546938225\n",
      "381-th epoch val loss 1.0885759200428042\n",
      "382-th epoch train loss 1.1145387316402644\n",
      "382-th epoch val loss 1.0886666682304396\n",
      "383-th epoch train loss 1.114605104759498\n",
      "383-th epoch val loss 1.0887563448764337\n",
      "384-th epoch train loss 1.1146707743351363\n",
      "384-th epoch val loss 1.0888449559361049\n",
      "385-th epoch train loss 1.114735740979375\n",
      "385-th epoch val loss 1.088932507607138\n",
      "386-th epoch train loss 1.1148000056175151\n",
      "386-th epoch val loss 1.0890190063153784\n",
      "387-th epoch train loss 1.114863569473048\n",
      "387-th epoch val loss 1.0891044587011656\n",
      "388-th epoch train loss 1.1149264340532807\n",
      "388-th epoch val loss 1.0891888716062026\n",
      "389-th epoch train loss 1.114988601135495\n",
      "389-th epoch val loss 1.0892722520609257\n",
      "390-th epoch train loss 1.1150500727536075\n",
      "390-th epoch val loss 1.0893546072723719\n",
      "391-th epoch train loss 1.1151108511853258\n",
      "391-th epoch val loss 1.08943594461252\n",
      "392-th epoch train loss 1.115170938939779\n",
      "392-th epoch val loss 1.089516271607087\n",
      "393-th epoch train loss 1.1152303387456046\n",
      "393-th epoch val loss 1.089595595924772\n",
      "394-th epoch train loss 1.115289053539484\n",
      "394-th epoch val loss 1.0896739253669248\n",
      "395-th epoch train loss 1.1153470864551034\n",
      "395-th epoch val loss 1.089751267857629\n",
      "396-th epoch train loss 1.1154044408125268\n",
      "396-th epoch val loss 1.089827631434181\n",
      "397-th epoch train loss 1.1154611201079778\n",
      "397-th epoch val loss 1.0899030242379593\n",
      "398-th epoch train loss 1.1155171280039973\n",
      "398-th epoch val loss 1.0899774545056562\n",
      "399-th epoch train loss 1.1155724683199835\n",
      "399-th epoch val loss 1.0900509305608805\n",
      "400-th epoch train loss 1.1156271450230892\n",
      "400-th epoch val loss 1.0901234608060935\n",
      "401-th epoch train loss 1.1156811622194667\n",
      "401-th epoch val loss 1.0901950537148872\n",
      "402-th epoch train loss 1.1157345241458545\n",
      "402-th epoch val loss 1.0902657178245838\n",
      "403-th epoch train loss 1.1157872351614826\n",
      "403-th epoch val loss 1.0903354617291423\n",
      "404-th epoch train loss 1.1158392997402988\n",
      "404-th epoch val loss 1.0904042940723695\n",
      "405-th epoch train loss 1.1158907224634935\n",
      "405-th epoch val loss 1.0904722235414193\n",
      "406-th epoch train loss 1.1159415080123227\n",
      "406-th epoch val loss 1.0905392588605693\n",
      "407-th epoch train loss 1.1159916611612082\n",
      "407-th epoch val loss 1.0906054087852701\n",
      "408-th epoch train loss 1.1160411867711144\n",
      "408-th epoch val loss 1.0906706820964516\n",
      "409-th epoch train loss 1.1160900897831925\n",
      "409-th epoch val loss 1.0907350875950819\n",
      "410-th epoch train loss 1.116138375212671\n",
      "410-th epoch val loss 1.0907986340969664\n",
      "411-th epoch train loss 1.1161860481430004\n",
      "411-th epoch val loss 1.090861330427782\n",
      "412-th epoch train loss 1.1162331137202328\n",
      "412-th epoch val loss 1.0909231854183337\n",
      "413-th epoch train loss 1.1162795771476282\n",
      "413-th epoch val loss 1.090984207900028\n",
      "414-th epoch train loss 1.1163254436804853\n",
      "414-th epoch val loss 1.0910444067005591\n",
      "415-th epoch train loss 1.1163707186211844\n",
      "415-th epoch val loss 1.0911037906397894\n",
      "416-th epoch train loss 1.1164154073144383\n",
      "416-th epoch val loss 1.0911623685258294\n",
      "417-th epoch train loss 1.1164595151427383\n",
      "417-th epoch val loss 1.0912201491513038\n",
      "418-th epoch train loss 1.1165030475219975\n",
      "418-th epoch val loss 1.091277141289796\n",
      "419-th epoch train loss 1.1165460098973732\n",
      "419-th epoch val loss 1.0913333536924654\n",
      "420-th epoch train loss 1.1165884077392743\n",
      "420-th epoch val loss 1.0913887950848356\n",
      "421-th epoch train loss 1.1166302465395355\n",
      "421-th epoch val loss 1.0914434741637404\n",
      "422-th epoch train loss 1.1166715318077605\n",
      "422-th epoch val loss 1.091497399594425\n",
      "423-th epoch train loss 1.116712269067827\n",
      "423-th epoch val loss 1.0915505800077974\n",
      "424-th epoch train loss 1.1167524638545414\n",
      "424-th epoch val loss 1.091603023997822\n",
      "425-th epoch train loss 1.1167921217104488\n",
      "425-th epoch val loss 1.091654740119056\n",
      "426-th epoch train loss 1.1168312481827816\n",
      "426-th epoch val loss 1.0917057368843115\n",
      "427-th epoch train loss 1.1168698488205497\n",
      "427-th epoch val loss 1.0917560227624517\n",
      "428-th epoch train loss 1.1169079291717634\n",
      "428-th epoch val loss 1.0918056061763104\n",
      "429-th epoch train loss 1.1169454947807849\n",
      "429-th epoch val loss 1.0918544955007239\n",
      "430-th epoch train loss 1.1169825511858045\n",
      "430-th epoch val loss 1.0919026990606833\n",
      "431-th epoch train loss 1.1170191039164343\n",
      "431-th epoch val loss 1.0919502251295956\n",
      "432-th epoch train loss 1.1170551584914226\n",
      "432-th epoch val loss 1.0919970819276479\n",
      "433-th epoch train loss 1.1170907204164715\n",
      "433-th epoch val loss 1.0920432776202713\n",
      "434-th epoch train loss 1.1171257951821687\n",
      "434-th epoch val loss 1.0920888203167103\n",
      "435-th epoch train loss 1.1171603882620178\n",
      "435-th epoch val loss 1.0921337180686759\n",
      "436-th epoch train loss 1.1171945051105694\n",
      "436-th epoch val loss 1.0921779788690966\n",
      "437-th epoch train loss 1.117228151161649\n",
      "437-th epoch val loss 1.0922216106509501\n",
      "438-th epoch train loss 1.1172613318266735\n",
      "438-th epoch val loss 1.092264621286181\n",
      "439-th epoch train loss 1.1172940524930597\n",
      "439-th epoch val loss 1.0923070185846986\n",
      "440-th epoch train loss 1.1173263185227165\n",
      "440-th epoch val loss 1.0923488102934498\n",
      "441-th epoch train loss 1.117358135250619\n",
      "441-th epoch val loss 1.0923900040955636\n",
      "442-th epoch train loss 1.1173895079834626\n",
      "442-th epoch val loss 1.092430607609571\n",
      "443-th epoch train loss 1.1174204419983917\n",
      "443-th epoch val loss 1.092470628388688\n",
      "444-th epoch train loss 1.1174509425418018\n",
      "444-th epoch val loss 1.0925100739201659\n",
      "445-th epoch train loss 1.1174810148282126\n",
      "445-th epoch val loss 1.092548951624702\n",
      "446-th epoch train loss 1.1175106640392098\n",
      "446-th epoch val loss 1.0925872688559117\n",
      "447-th epoch train loss 1.117539895322447\n",
      "447-th epoch val loss 1.0926250328998535\n",
      "448-th epoch train loss 1.117568713790715\n",
      "448-th epoch val loss 1.0926622509746162\n",
      "449-th epoch train loss 1.117597124521072\n",
      "449-th epoch val loss 1.0926989302299506\n",
      "450-th epoch train loss 1.1176251325540232\n",
      "450-th epoch val loss 1.0927350777469544\n",
      "451-th epoch train loss 1.117652742892767\n",
      "451-th epoch val loss 1.0927707005378073\n",
      "452-th epoch train loss 1.1176799605024856\n",
      "452-th epoch val loss 1.0928058055455478\n",
      "453-th epoch train loss 1.1177067903096913\n",
      "453-th epoch val loss 1.0928403996438956\n",
      "454-th epoch train loss 1.1177332372016209\n",
      "454-th epoch val loss 1.0928744896371183\n",
      "455-th epoch train loss 1.1177593060256745\n",
      "455-th epoch val loss 1.092908082259932\n",
      "456-th epoch train loss 1.1177850015889048\n",
      "456-th epoch val loss 1.0929411841774475\n",
      "457-th epoch train loss 1.1178103286575458\n",
      "457-th epoch val loss 1.0929738019851478\n",
      "458-th epoch train loss 1.1178352919565817\n",
      "458-th epoch val loss 1.0930059422089005\n",
      "459-th epoch train loss 1.117859896169361\n",
      "459-th epoch val loss 1.0930376113050067\n",
      "460-th epoch train loss 1.1178841459372446\n",
      "460-th epoch val loss 1.0930688156602804\n",
      "461-th epoch train loss 1.1179080458592903\n",
      "461-th epoch val loss 1.0930995615921533\n",
      "462-th epoch train loss 1.1179316004919755\n",
      "462-th epoch val loss 1.0931298553488165\n",
      "463-th epoch train loss 1.117954814348951\n",
      "463-th epoch val loss 1.0931597031093827\n",
      "464-th epoch train loss 1.1179776919008273\n",
      "464-th epoch val loss 1.0931891109840777\n",
      "465-th epoch train loss 1.118000237574992\n",
      "465-th epoch val loss 1.093218085014455\n",
      "466-th epoch train loss 1.118022455755457\n",
      "466-th epoch val loss 1.0932466311736357\n",
      "467-th epoch train loss 1.1180443507827331\n",
      "467-th epoch val loss 1.0932747553665678\n",
      "468-th epoch train loss 1.1180659269537319\n",
      "468-th epoch val loss 1.0933024634303081\n",
      "469-th epoch train loss 1.1180871885216923\n",
      "469-th epoch val loss 1.0933297611343267\n",
      "470-th epoch train loss 1.1181081396961356\n",
      "470-th epoch val loss 1.0933566541808242\n",
      "471-th epoch train loss 1.118128784642838\n",
      "471-th epoch val loss 1.0933831482050724\n",
      "472-th epoch train loss 1.1181491274838289\n",
      "472-th epoch val loss 1.09340924877577\n",
      "473-th epoch train loss 1.1181691722974128\n",
      "473-th epoch val loss 1.093434961395413\n",
      "474-th epoch train loss 1.1181889231182043\n",
      "474-th epoch val loss 1.0934602915006808\n",
      "475-th epoch train loss 1.118208383937192\n",
      "475-th epoch val loss 1.093485244462837\n",
      "476-th epoch train loss 1.1182275587018125\n",
      "476-th epoch val loss 1.0935098255881446\n",
      "477-th epoch train loss 1.1182464513160477\n",
      "477-th epoch val loss 1.09353404011829\n",
      "478-th epoch train loss 1.1182650656405344\n",
      "478-th epoch val loss 1.0935578932308219\n",
      "479-th epoch train loss 1.1182834054926936\n",
      "479-th epoch val loss 1.0935813900396003\n",
      "480-th epoch train loss 1.1183014746468725\n",
      "480-th epoch val loss 1.0936045355952544\n",
      "481-th epoch train loss 1.1183192768345014\n",
      "481-th epoch val loss 1.093627334885653\n",
      "482-th epoch train loss 1.1183368157442657\n",
      "482-th epoch val loss 1.0936497928363806\n",
      "483-th epoch train loss 1.118354095022287\n",
      "483-th epoch val loss 1.0936719143112217\n",
      "484-th epoch train loss 1.1183711182723208\n",
      "484-th epoch val loss 1.0936937041126567\n",
      "485-th epoch train loss 1.1183878890559635\n",
      "485-th epoch val loss 1.0937151669823595\n",
      "486-th epoch train loss 1.11840441089287\n",
      "486-th epoch val loss 1.0937363076017061\n",
      "487-th epoch train loss 1.118420687260981\n",
      "487-th epoch val loss 1.0937571305922842\n",
      "488-th epoch train loss 1.1184367215967645\n",
      "488-th epoch val loss 1.0937776405164144\n",
      "489-th epoch train loss 1.1184525172954594\n",
      "489-th epoch val loss 1.0937978418776697\n",
      "490-th epoch train loss 1.118468077711333\n",
      "490-th epoch val loss 1.093817739121405\n",
      "491-th epoch train loss 1.1184834061579463\n",
      "491-th epoch val loss 1.093837336635286\n",
      "492-th epoch train loss 1.1184985059084225\n",
      "492-th epoch val loss 1.0938566387498254\n",
      "493-th epoch train loss 1.118513380195728\n",
      "493-th epoch val loss 1.0938756497389188\n",
      "494-th epoch train loss 1.118528032212958\n",
      "494-th epoch val loss 1.0938943738203866\n",
      "495-th epoch train loss 1.1185424651136242\n",
      "495-th epoch val loss 1.093912815156514\n",
      "496-th epoch train loss 1.118556682011957\n",
      "496-th epoch val loss 1.0939309778545971\n",
      "497-th epoch train loss 1.1185706859832045\n",
      "497-th epoch val loss 1.0939488659674899\n",
      "498-th epoch train loss 1.1185844800639415\n",
      "498-th epoch val loss 1.0939664834941476\n",
      "499-th epoch train loss 1.1185980672523819\n",
      "499-th epoch val loss 1.0939838343801798\n",
      "500-th epoch train loss 1.1186114505086924\n",
      "500-th epoch val loss 1.0940009225183924\n",
      "501-th epoch train loss 1.1186246327553169\n",
      "501-th epoch val loss 1.0940177517493443\n",
      "502-th epoch train loss 1.118637616877296\n",
      "502-th epoch val loss 1.0940343258618908\n",
      "503-th epoch train loss 1.1186504057225966\n",
      "503-th epoch val loss 1.0940506485937322\n",
      "504-th epoch train loss 1.1186630021024409\n",
      "504-th epoch val loss 1.094066723631966\n",
      "505-th epoch train loss 1.118675408791638\n",
      "505-th epoch val loss 1.094082554613631\n",
      "506-th epoch train loss 1.1186876285289202\n",
      "506-th epoch val loss 1.094098145126254\n",
      "507-th epoch train loss 1.1186996640172782\n",
      "507-th epoch val loss 1.0941134987083965\n",
      "508-th epoch train loss 1.1187115179243006\n",
      "508-th epoch val loss 1.0941286188501973\n",
      "509-th epoch train loss 1.1187231928825148\n",
      "509-th epoch val loss 1.0941435089939149\n",
      "510-th epoch train loss 1.1187346914897252\n",
      "510-th epoch val loss 1.094158172534468\n",
      "511-th epoch train loss 1.1187460163093605\n",
      "511-th epoch val loss 1.0941726128199751\n",
      "512-th epoch train loss 1.1187571698708123\n",
      "512-th epoch val loss 1.0941868331522877\n",
      "513-th epoch train loss 1.118768154669784\n",
      "513-th epoch val loss 1.0942008367875287\n",
      "514-th epoch train loss 1.1187789731686302\n",
      "514-th epoch val loss 1.0942146269366184\n",
      "515-th epoch train loss 1.118789627796706\n",
      "515-th epoch val loss 1.0942282067658096\n",
      "516-th epoch train loss 1.1188001209507088\n",
      "516-th epoch val loss 1.094241579397209\n",
      "517-th epoch train loss 1.1188104549950249\n",
      "517-th epoch val loss 1.0942547479093014\n",
      "518-th epoch train loss 1.118820632262073\n",
      "518-th epoch val loss 1.0942677153374731\n",
      "519-th epoch train loss 1.1188306550526501\n",
      "519-th epoch val loss 1.0942804846745244\n",
      "520-th epoch train loss 1.1188405256362723\n",
      "520-th epoch val loss 1.0942930588711877\n",
      "521-th epoch train loss 1.1188502462515209\n",
      "521-th epoch val loss 1.0943054408366346\n",
      "522-th epoch train loss 1.118859819106382\n",
      "522-th epoch val loss 1.0943176334389866\n",
      "523-th epoch train loss 1.1188692463785892\n",
      "523-th epoch val loss 1.094329639505817\n",
      "524-th epoch train loss 1.1188785302159632\n",
      "524-th epoch val loss 1.0943414618246499\n",
      "525-th epoch train loss 1.1188876727367507\n",
      "525-th epoch val loss 1.0943531031434595\n",
      "526-th epoch train loss 1.118896676029962\n",
      "526-th epoch val loss 1.0943645661711603\n",
      "527-th epoch train loss 1.1189055421557081\n",
      "527-th epoch val loss 1.094375853578099\n",
      "528-th epoch train loss 1.118914273145534\n",
      "528-th epoch val loss 1.0943869679965363\n",
      "529-th epoch train loss 1.1189228710027521\n",
      "529-th epoch val loss 1.0943979120211302\n",
      "530-th epoch train loss 1.1189313377027752\n",
      "530-th epoch val loss 1.094408688209413\n",
      "531-th epoch train loss 1.1189396751934453\n",
      "531-th epoch val loss 1.0944192990822639\n",
      "532-th epoch train loss 1.1189478853953596\n",
      "532-th epoch val loss 1.0944297471243785\n",
      "533-th epoch train loss 1.1189559702021994\n",
      "533-th epoch val loss 1.094440034784734\n",
      "534-th epoch train loss 1.1189639314810527\n",
      "534-th epoch val loss 1.0944501644770501\n",
      "535-th epoch train loss 1.1189717710727356\n",
      "535-th epoch val loss 1.0944601385802457\n",
      "536-th epoch train loss 1.1189794907921133\n",
      "536-th epoch val loss 1.0944699594388907\n",
      "537-th epoch train loss 1.118987092428417\n",
      "537-th epoch val loss 1.0944796293636556\n",
      "538-th epoch train loss 1.118994577745559\n",
      "538-th epoch val loss 1.0944891506317542\n",
      "539-th epoch train loss 1.1190019484824452\n",
      "539-th epoch val loss 1.094498525487385\n",
      "540-th epoch train loss 1.1190092063532893\n",
      "540-th epoch val loss 1.094507756142165\n",
      "541-th epoch train loss 1.1190163530479167\n",
      "541-th epoch val loss 1.0945168447755624\n",
      "542-th epoch train loss 1.1190233902320736\n",
      "542-th epoch val loss 1.0945257935353228\n",
      "543-th epoch train loss 1.1190303195477291\n",
      "543-th epoch val loss 1.0945346045378916\n",
      "544-th epoch train loss 1.119037142613377\n",
      "544-th epoch val loss 1.094543279868831\n",
      "545-th epoch train loss 1.1190438610243347\n",
      "545-th epoch val loss 1.0945518215832393\n",
      "546-th epoch train loss 1.1190504763530376\n",
      "546-th epoch val loss 1.0945602317061525\n",
      "547-th epoch train loss 1.1190569901493344\n",
      "547-th epoch val loss 1.094568512232956\n",
      "548-th epoch train loss 1.1190634039407774\n",
      "548-th epoch val loss 1.0945766651297828\n",
      "549-th epoch train loss 1.1190697192329104\n",
      "549-th epoch val loss 1.0945846923339098\n",
      "550-th epoch train loss 1.119075937509555\n",
      "550-th epoch val loss 1.0945925957541525\n",
      "551-th epoch train loss 1.1190820602330922\n",
      "551-th epoch val loss 1.09460037727125\n",
      "552-th epoch train loss 1.1190880888447452\n",
      "552-th epoch val loss 1.0946080387382506\n",
      "553-th epoch train loss 1.119094024764854\n",
      "553-th epoch val loss 1.094615581980891\n",
      "554-th epoch train loss 1.1190998693931538\n",
      "554-th epoch val loss 1.0946230087979716\n",
      "555-th epoch train loss 1.1191056241090445\n",
      "555-th epoch val loss 1.0946303209617263\n",
      "556-th epoch train loss 1.119111290271861\n",
      "556-th epoch val loss 1.094637520218191\n",
      "557-th epoch train loss 1.1191168692211408\n",
      "557-th epoch val loss 1.0946446082875645\n",
      "558-th epoch train loss 1.1191223622768873\n",
      "558-th epoch val loss 1.0946515868645672\n",
      "559-th epoch train loss 1.1191277707398324\n",
      "559-th epoch val loss 1.0946584576187957\n",
      "560-th epoch train loss 1.1191330958916936\n",
      "560-th epoch val loss 1.0946652221950728\n",
      "561-th epoch train loss 1.1191383389954308\n",
      "561-th epoch val loss 1.0946718822137922\n",
      "562-th epoch train loss 1.1191435012955007\n",
      "562-th epoch val loss 1.094678439271261\n",
      "563-th epoch train loss 1.1191485840181044\n",
      "563-th epoch val loss 1.0946848949400383\n",
      "564-th epoch train loss 1.1191535883714387\n",
      "564-th epoch val loss 1.0946912507692659\n",
      "565-th epoch train loss 1.1191585155459391\n",
      "565-th epoch val loss 1.0946975082850008\n",
      "566-th epoch train loss 1.1191633667145222\n",
      "566-th epoch val loss 1.0947036689905387\n",
      "567-th epoch train loss 1.119168143032827\n",
      "567-th epoch val loss 1.0947097343667371\n",
      "568-th epoch train loss 1.119172845639451\n",
      "568-th epoch val loss 1.0947157058723314\n",
      "569-th epoch train loss 1.119177475656183\n",
      "569-th epoch val loss 1.0947215849442489\n",
      "570-th epoch train loss 1.1191820341882384\n",
      "570-th epoch val loss 1.0947273729979192\n",
      "571-th epoch train loss 1.1191865223244855\n",
      "571-th epoch val loss 1.0947330714275791\n",
      "572-th epoch train loss 1.1191909411376726\n",
      "572-th epoch val loss 1.0947386816065758\n",
      "573-th epoch train loss 1.1191952916846508\n",
      "573-th epoch val loss 1.0947442048876634\n",
      "574-th epoch train loss 1.1191995750065986\n",
      "574-th epoch val loss 1.0947496426032992\n",
      "575-th epoch train loss 1.119203792129237\n",
      "575-th epoch val loss 1.094754996065933\n",
      "576-th epoch train loss 1.1192079440630454\n",
      "576-th epoch val loss 1.0947602665682945\n",
      "577-th epoch train loss 1.1192120318034782\n",
      "577-th epoch val loss 1.094765455383677\n",
      "578-th epoch train loss 1.1192160563311715\n",
      "578-th epoch val loss 1.0947705637662164\n",
      "579-th epoch train loss 1.1192200186121553\n",
      "579-th epoch val loss 1.0947755929511676\n",
      "580-th epoch train loss 1.1192239195980571\n",
      "580-th epoch val loss 1.0947805441551768\n",
      "581-th epoch train loss 1.119227760226304\n",
      "581-th epoch val loss 1.0947854185765509\n",
      "582-th epoch train loss 1.1192315414203273\n",
      "582-th epoch val loss 1.0947902173955213\n",
      "583-th epoch train loss 1.1192352640897567\n",
      "583-th epoch val loss 1.094794941774508\n",
      "584-th epoch train loss 1.1192389291306188\n",
      "584-th epoch val loss 1.0947995928583762\n",
      "585-th epoch train loss 1.1192425374255282\n",
      "585-th epoch val loss 1.0948041717746928\n",
      "586-th epoch train loss 1.1192460898438812\n",
      "586-th epoch val loss 1.0948086796339764\n",
      "587-th epoch train loss 1.119249587242042\n",
      "587-th epoch val loss 1.0948131175299465\n",
      "588-th epoch train loss 1.1192530304635306\n",
      "588-th epoch val loss 1.09481748653977\n",
      "589-th epoch train loss 1.1192564203392057\n",
      "589-th epoch val loss 1.0948217877242996\n",
      "590-th epoch train loss 1.1192597576874457\n",
      "590-th epoch val loss 1.0948260221283155\n",
      "591-th epoch train loss 1.119263043314328\n",
      "591-th epoch val loss 1.0948301907807574\n",
      "592-th epoch train loss 1.1192662780138078\n",
      "592-th epoch val loss 1.0948342946949599\n",
      "593-th epoch train loss 1.1192694625678894\n",
      "593-th epoch val loss 1.094838334868879\n",
      "594-th epoch train loss 1.1192725977468017\n",
      "594-th epoch val loss 1.0948423122853201\n",
      "595-th epoch train loss 1.1192756843091651\n",
      "595-th epoch val loss 1.0948462279121574\n",
      "596-th epoch train loss 1.1192787230021626\n",
      "596-th epoch val loss 1.0948500827025573\n",
      "597-th epoch train loss 1.119281714561703\n",
      "597-th epoch val loss 1.0948538775951933\n",
      "598-th epoch train loss 1.1192846597125852\n",
      "598-th epoch val loss 1.0948576135144588\n",
      "599-th epoch train loss 1.1192875591686597\n",
      "599-th epoch val loss 1.0948612913706808\n",
      "600-th epoch train loss 1.119290413632987\n",
      "600-th epoch val loss 1.0948649120603235\n",
      "601-th epoch train loss 1.119293223797996\n",
      "601-th epoch val loss 1.0948684764661982\n",
      "602-th epoch train loss 1.1192959903456372\n",
      "602-th epoch val loss 1.0948719854576612\n",
      "603-th epoch train loss 1.1192987139475377\n",
      "603-th epoch val loss 1.094875439890816\n",
      "604-th epoch train loss 1.1193013952651494\n",
      "604-th epoch val loss 1.0948788406087087\n",
      "605-th epoch train loss 1.1193040349499002\n",
      "605-th epoch val loss 1.094882188441521\n",
      "606-th epoch train loss 1.1193066336433404\n",
      "606-th epoch val loss 1.0948854842067641\n",
      "607-th epoch train loss 1.1193091919772853\n",
      "607-th epoch val loss 1.0948887287094637\n",
      "608-th epoch train loss 1.1193117105739618\n",
      "608-th epoch val loss 1.0948919227423481\n",
      "609-th epoch train loss 1.1193141900461454\n",
      "609-th epoch val loss 1.0948950670860322\n",
      "610-th epoch train loss 1.1193166309973022\n",
      "610-th epoch val loss 1.094898162509194\n",
      "611-th epoch train loss 1.1193190340217247\n",
      "611-th epoch val loss 1.0949012097687576\n",
      "612-th epoch train loss 1.119321399704667\n",
      "612-th epoch val loss 1.0949042096100654\n",
      "613-th epoch train loss 1.1193237286224778\n",
      "613-th epoch val loss 1.0949071627670524\n",
      "614-th epoch train loss 1.1193260213427334\n",
      "614-th epoch val loss 1.0949100699624177\n",
      "615-th epoch train loss 1.1193282784243659\n",
      "615-th epoch val loss 1.094912931907791\n",
      "616-th epoch train loss 1.1193305004177916\n",
      "616-th epoch val loss 1.0949157493038988\n",
      "617-th epoch train loss 1.1193326878650371\n",
      "617-th epoch val loss 1.0949185228407283\n",
      "618-th epoch train loss 1.1193348412998647\n",
      "618-th epoch val loss 1.0949212531976888\n",
      "619-th epoch train loss 1.1193369612478923\n",
      "619-th epoch val loss 1.0949239410437692\n",
      "620-th epoch train loss 1.119339048226718\n",
      "620-th epoch val loss 1.0949265870376967\n",
      "621-th epoch train loss 1.1193411027460358\n",
      "621-th epoch val loss 1.0949291918280877\n",
      "622-th epoch train loss 1.1193431253077564\n",
      "622-th epoch val loss 1.0949317560536034\n",
      "623-th epoch train loss 1.11934511640612\n",
      "623-th epoch val loss 1.094934280343097\n",
      "624-th epoch train loss 1.119347076527814\n",
      "624-th epoch val loss 1.094936765315764\n",
      "625-th epoch train loss 1.119349006152083\n",
      "625-th epoch val loss 1.094939211581284\n",
      "626-th epoch train loss 1.1193509057508388\n",
      "626-th epoch val loss 1.0949416197399677\n",
      "627-th epoch train loss 1.1193527757887753\n",
      "627-th epoch val loss 1.094943990382898\n",
      "628-th epoch train loss 1.119354616723471\n",
      "628-th epoch val loss 1.094946324092066\n",
      "629-th epoch train loss 1.1193564290054971\n",
      "629-th epoch val loss 1.094948621440512\n",
      "630-th epoch train loss 1.1193582130785247\n",
      "630-th epoch val loss 1.0949508829924597\n",
      "631-th epoch train loss 1.1193599693794232\n",
      "631-th epoch val loss 1.0949531093034477\n",
      "632-th epoch train loss 1.1193616983383687\n",
      "632-th epoch val loss 1.0949553009204647\n",
      "633-th epoch train loss 1.1193634003789386\n",
      "633-th epoch val loss 1.0949574583820747\n",
      "634-th epoch train loss 1.119365075918214\n",
      "634-th epoch val loss 1.0949595822185476\n",
      "635-th epoch train loss 1.1193667253668769\n",
      "635-th epoch val loss 1.0949616729519833\n",
      "636-th epoch train loss 1.119368349129305\n",
      "636-th epoch val loss 1.0949637310964375\n",
      "637-th epoch train loss 1.1193699476036685\n",
      "637-th epoch val loss 1.0949657571580407\n",
      "638-th epoch train loss 1.1193715211820219\n",
      "638-th epoch val loss 1.094967751635122\n",
      "639-th epoch train loss 1.119373070250397\n",
      "639-th epoch val loss 1.094969715018325\n",
      "640-th epoch train loss 1.1193745951888938\n",
      "640-th epoch val loss 1.0949716477907248\n",
      "641-th epoch train loss 1.1193760963717698\n",
      "641-th epoch val loss 1.0949735504279456\n",
      "642-th epoch train loss 1.1193775741675271\n",
      "642-th epoch val loss 1.0949754233982696\n",
      "643-th epoch train loss 1.1193790289390009\n",
      "643-th epoch val loss 1.094977267162753\n",
      "644-th epoch train loss 1.1193804610434452\n",
      "644-th epoch val loss 1.0949790821753338\n",
      "645-th epoch train loss 1.119381870832616\n",
      "645-th epoch val loss 1.094980868882941\n",
      "646-th epoch train loss 1.1193832586528536\n",
      "646-th epoch val loss 1.0949826277256\n",
      "647-th epoch train loss 1.1193846248451682\n",
      "647-th epoch val loss 1.0949843591365394\n",
      "648-th epoch train loss 1.1193859697453166\n",
      "648-th epoch val loss 1.0949860635422937\n",
      "649-th epoch train loss 1.119387293683885\n",
      "649-th epoch val loss 1.0949877413628062\n",
      "650-th epoch train loss 1.119388596986364\n",
      "650-th epoch val loss 1.0949893930115304\n",
      "651-th epoch train loss 1.1193898799732298\n",
      "651-th epoch val loss 1.0949910188955254\n",
      "652-th epoch train loss 1.1193911429600178\n",
      "652-th epoch val loss 1.0949926194155573\n",
      "653-th epoch train loss 1.1193923862573971\n",
      "653-th epoch val loss 1.0949941949661948\n",
      "654-th epoch train loss 1.119393610171247\n",
      "654-th epoch val loss 1.0949957459359019\n",
      "655-th epoch train loss 1.119394815002727\n",
      "655-th epoch val loss 1.094997272707133\n",
      "656-th epoch train loss 1.1193960010483506\n",
      "656-th epoch val loss 1.0949987756564252\n",
      "657-th epoch train loss 1.1193971686000543\n",
      "657-th epoch val loss 1.0950002551544862\n",
      "658-th epoch train loss 1.1193983179452698\n",
      "658-th epoch val loss 1.0950017115662887\n",
      "659-th epoch train loss 1.1193994493669892\n",
      "659-th epoch val loss 1.095003145251151\n",
      "660-th epoch train loss 1.1194005631438357\n",
      "660-th epoch val loss 1.09500455656283\n",
      "661-th epoch train loss 1.1194016595501277\n",
      "661-th epoch val loss 1.095005945849603\n",
      "662-th epoch train loss 1.1194027388559469\n",
      "662-th epoch val loss 1.0950073134543536\n",
      "663-th epoch train loss 1.1194038013272005\n",
      "663-th epoch val loss 1.0950086597146533\n",
      "664-th epoch train loss 1.1194048472256868\n",
      "664-th epoch val loss 1.0950099849628419\n",
      "665-th epoch train loss 1.1194058768091562\n",
      "665-th epoch val loss 1.095011289526109\n",
      "666-th epoch train loss 1.1194068903313745\n",
      "666-th epoch val loss 1.0950125737265757\n",
      "667-th epoch train loss 1.1194078880421836\n",
      "667-th epoch val loss 1.0950138378813665\n",
      "668-th epoch train loss 1.1194088701875606\n",
      "668-th epoch val loss 1.0950150823026912\n",
      "669-th epoch train loss 1.119409837009677\n",
      "669-th epoch val loss 1.0950163072979178\n",
      "670-th epoch train loss 1.1194107887469584\n",
      "670-th epoch val loss 1.0950175131696487\n",
      "671-th epoch train loss 1.1194117256341396\n",
      "671-th epoch val loss 1.0950187002157914\n",
      "672-th epoch train loss 1.119412647902323\n",
      "672-th epoch val loss 1.095019868729633\n",
      "673-th epoch train loss 1.119413555779032\n",
      "673-th epoch val loss 1.0950210189999108\n",
      "674-th epoch train loss 1.1194144494882692\n",
      "674-th epoch val loss 1.0950221513108807\n",
      "675-th epoch train loss 1.1194153292505662\n",
      "675-th epoch val loss 1.0950232659423875\n",
      "676-th epoch train loss 1.119416195283041\n",
      "676-th epoch val loss 1.0950243631699348\n",
      "677-th epoch train loss 1.119417047799447\n",
      "677-th epoch val loss 1.0950254432647466\n",
      "678-th epoch train loss 1.1194178870102263\n",
      "678-th epoch val loss 1.0950265064938394\n",
      "679-th epoch train loss 1.1194187131225593\n",
      "679-th epoch val loss 1.0950275531200815\n",
      "680-th epoch train loss 1.1194195263404172\n",
      "680-th epoch val loss 1.0950285834022608\n",
      "681-th epoch train loss 1.1194203268646084\n",
      "681-th epoch val loss 1.0950295975951456\n",
      "682-th epoch train loss 1.1194211148928281\n",
      "682-th epoch val loss 1.0950305959495468\n",
      "683-th epoch train loss 1.1194218906197069\n",
      "683-th epoch val loss 1.0950315787123808\n",
      "684-th epoch train loss 1.119422654236857\n",
      "684-th epoch val loss 1.095032546126727\n",
      "685-th epoch train loss 1.119423405932919\n",
      "685-th epoch val loss 1.0950334984318877\n",
      "686-th epoch train loss 1.1194241458936072\n",
      "686-th epoch val loss 1.0950344358634483\n",
      "687-th epoch train loss 1.1194248743017547\n",
      "687-th epoch val loss 1.095035358653331\n",
      "688-th epoch train loss 1.1194255913373587\n",
      "688-th epoch val loss 1.0950362670298555\n",
      "689-th epoch train loss 1.1194262971776225\n",
      "689-th epoch val loss 1.0950371612177914\n",
      "690-th epoch train loss 1.1194269919969997\n",
      "690-th epoch val loss 1.0950380414384149\n",
      "691-th epoch train loss 1.119427675967236\n",
      "691-th epoch val loss 1.0950389079095608\n",
      "692-th epoch train loss 1.1194283492574109\n",
      "692-th epoch val loss 1.0950397608456794\n",
      "693-th epoch train loss 1.1194290120339783\n",
      "693-th epoch val loss 1.0950406004578834\n",
      "694-th epoch train loss 1.1194296644608095\n",
      "694-th epoch val loss 1.0950414269540043\n",
      "695-th epoch train loss 1.119430306699229\n",
      "695-th epoch val loss 1.095042240538641\n",
      "696-th epoch train loss 1.119430938908057\n",
      "696-th epoch val loss 1.0950430414132097\n",
      "697-th epoch train loss 1.1194315612436458\n",
      "697-th epoch val loss 1.095043829775994\n",
      "698-th epoch train loss 1.1194321738599202\n",
      "698-th epoch val loss 1.0950446058221917\n",
      "699-th epoch train loss 1.1194327769084123\n",
      "699-th epoch val loss 1.0950453697439644\n",
      "700-th epoch train loss 1.1194333705383004\n",
      "700-th epoch val loss 1.0950461217304843\n",
      "701-th epoch train loss 1.1194339548964447\n",
      "701-th epoch val loss 1.0950468619679785\n",
      "702-th epoch train loss 1.1194345301274229\n",
      "702-th epoch val loss 1.0950475906397774\n",
      "703-th epoch train loss 1.1194350963735653\n",
      "703-th epoch val loss 1.0950483079263567\n",
      "704-th epoch train loss 1.1194356537749903\n",
      "704-th epoch val loss 1.0950490140053846\n",
      "705-th epoch train loss 1.1194362024696372\n",
      "705-th epoch val loss 1.095049709051762\n",
      "706-th epoch train loss 1.1194367425933014\n",
      "706-th epoch val loss 1.0950503932376685\n",
      "707-th epoch train loss 1.1194372742796672\n",
      "707-th epoch val loss 1.0950510667326017\n",
      "708-th epoch train loss 1.1194377976603382\n",
      "708-th epoch val loss 1.0950517297034215\n",
      "709-th epoch train loss 1.119438312864874\n",
      "709-th epoch val loss 1.0950523823143887\n",
      "710-th epoch train loss 1.1194388200208165\n",
      "710-th epoch val loss 1.0950530247272066\n",
      "711-th epoch train loss 1.119439319253725\n",
      "711-th epoch val loss 1.0950536571010607\n",
      "712-th epoch train loss 1.1194398106872059\n",
      "712-th epoch val loss 1.0950542795926559\n",
      "713-th epoch train loss 1.119440294442942\n",
      "713-th epoch val loss 1.0950548923562577\n",
      "714-th epoch train loss 1.1194407706407223\n",
      "714-th epoch val loss 1.0950554955437284\n",
      "715-th epoch train loss 1.1194412393984727\n",
      "715-th epoch val loss 1.0950560893045636\n",
      "716-th epoch train loss 1.1194417008322843\n",
      "716-th epoch val loss 1.0950566737859322\n",
      "717-th epoch train loss 1.1194421550564404\n",
      "717-th epoch val loss 1.0950572491327075\n",
      "718-th epoch train loss 1.1194426021834458\n",
      "718-th epoch val loss 1.0950578154875072\n",
      "719-th epoch train loss 1.119443042324055\n",
      "719-th epoch val loss 1.095058372990727\n",
      "720-th epoch train loss 1.119443475587297\n",
      "720-th epoch val loss 1.0950589217805742\n",
      "721-th epoch train loss 1.1194439020805047\n",
      "721-th epoch val loss 1.095059461993102\n",
      "722-th epoch train loss 1.11944432190934\n",
      "722-th epoch val loss 1.0950599937622447\n",
      "723-th epoch train loss 1.1194447351778174\n",
      "723-th epoch val loss 1.0950605172198469\n",
      "724-th epoch train loss 1.1194451419883344\n",
      "724-th epoch val loss 1.0950610324957\n",
      "725-th epoch train loss 1.1194455424416931\n",
      "725-th epoch val loss 1.0950615397175725\n",
      "726-th epoch train loss 1.1194459366371246\n",
      "726-th epoch val loss 1.0950620390112393\n",
      "727-th epoch train loss 1.119446324672316\n",
      "727-th epoch val loss 1.0950625305005164\n",
      "728-th epoch train loss 1.119446706643432\n",
      "728-th epoch val loss 1.0950630143072888\n",
      "729-th epoch train loss 1.119447082645139\n",
      "729-th epoch val loss 1.095063490551541\n",
      "730-th epoch train loss 1.11944745277063\n",
      "730-th epoch val loss 1.095063959351387\n",
      "731-th epoch train loss 1.1194478171116442\n",
      "731-th epoch val loss 1.0950644208230984\n",
      "732-th epoch train loss 1.1194481757584933\n",
      "732-th epoch val loss 1.0950648750811358\n",
      "733-th epoch train loss 1.119448528800081\n",
      "733-th epoch val loss 1.0950653222381728\n",
      "734-th epoch train loss 1.119448876323926\n",
      "734-th epoch val loss 1.0950657624051268\n",
      "735-th epoch train loss 1.1194492184161828\n",
      "735-th epoch val loss 1.095066195691185\n",
      "736-th epoch train loss 1.119449555161663\n",
      "736-th epoch val loss 1.0950666222038308\n",
      "737-th epoch train loss 1.1194498866438576\n",
      "737-th epoch val loss 1.0950670420488733\n",
      "738-th epoch train loss 1.1194502129449548\n",
      "738-th epoch val loss 1.09506745533047\n",
      "739-th epoch train loss 1.1194505341458627\n",
      "739-th epoch val loss 1.095067862151153\n",
      "740-th epoch train loss 1.1194508503262284\n",
      "740-th epoch val loss 1.0950682626118557\n",
      "741-th epoch train loss 1.1194511615644565\n",
      "741-th epoch val loss 1.0950686568119377\n",
      "742-th epoch train loss 1.1194514679377299\n",
      "742-th epoch val loss 1.0950690448492069\n",
      "743-th epoch train loss 1.1194517695220274\n",
      "743-th epoch val loss 1.0950694268199455\n",
      "744-th epoch train loss 1.1194520663921448\n",
      "744-th epoch val loss 1.0950698028189352\n",
      "745-th epoch train loss 1.1194523586217102\n",
      "745-th epoch val loss 1.0950701729394763\n",
      "746-th epoch train loss 1.119452646283204\n",
      "746-th epoch val loss 1.0950705372734157\n",
      "747-th epoch train loss 1.1194529294479776\n",
      "747-th epoch val loss 1.0950708959111657\n",
      "748-th epoch train loss 1.119453208186267\n",
      "748-th epoch val loss 1.0950712489417278\n",
      "749-th epoch train loss 1.1194534825672156\n",
      "749-th epoch val loss 1.095071596452716\n",
      "750-th epoch train loss 1.1194537526588868\n",
      "750-th epoch val loss 1.0950719385303738\n",
      "751-th epoch train loss 1.1194540185282826\n",
      "751-th epoch val loss 1.095072275259602\n",
      "752-th epoch train loss 1.1194542802413612\n",
      "752-th epoch val loss 1.095072606723975\n",
      "753-th epoch train loss 1.1194545378630498\n",
      "753-th epoch val loss 1.095072933005763\n",
      "754-th epoch train loss 1.1194547914572646\n",
      "754-th epoch val loss 1.0950732541859514\n",
      "755-th epoch train loss 1.1194550410869242\n",
      "755-th epoch val loss 1.0950735703442624\n",
      "756-th epoch train loss 1.119455286813965\n",
      "756-th epoch val loss 1.0950738815591738\n",
      "757-th epoch train loss 1.119455528699358\n",
      "757-th epoch val loss 1.0950741879079378\n",
      "758-th epoch train loss 1.1194557668031226\n",
      "758-th epoch val loss 1.0950744894666007\n",
      "759-th epoch train loss 1.1194560011843413\n",
      "759-th epoch val loss 1.095074786310021\n",
      "760-th epoch train loss 1.1194562319011756\n",
      "760-th epoch val loss 1.095075078511891\n",
      "761-th epoch train loss 1.119456459010879\n",
      "761-th epoch val loss 1.0950753661447499\n",
      "762-th epoch train loss 1.1194566825698116\n",
      "762-th epoch val loss 1.0950756492800058\n",
      "763-th epoch train loss 1.1194569026334535\n",
      "763-th epoch val loss 1.0950759279879507\n",
      "764-th epoch train loss 1.1194571192564204\n",
      "764-th epoch val loss 1.0950762023377805\n",
      "765-th epoch train loss 1.119457332492474\n",
      "765-th epoch val loss 1.0950764723976092\n",
      "766-th epoch train loss 1.1194575423945379\n",
      "766-th epoch val loss 1.0950767382344886\n",
      "767-th epoch train loss 1.1194577490147095\n",
      "767-th epoch val loss 1.0950769999144225\n",
      "768-th epoch train loss 1.1194579524042725\n",
      "768-th epoch val loss 1.0950772575023844\n",
      "769-th epoch train loss 1.1194581526137113\n",
      "769-th epoch val loss 1.0950775110623334\n",
      "770-th epoch train loss 1.1194583496927215\n",
      "770-th epoch val loss 1.095077760657229\n",
      "771-th epoch train loss 1.1194585436902231\n",
      "771-th epoch val loss 1.095078006349049\n",
      "772-th epoch train loss 1.119458734654372\n",
      "772-th epoch val loss 1.0950782481988013\n",
      "773-th epoch train loss 1.1194589226325737\n",
      "773-th epoch val loss 1.0950784862665426\n",
      "774-th epoch train loss 1.1194591076714924\n",
      "774-th epoch val loss 1.095078720611391\n",
      "775-th epoch train loss 1.1194592898170634\n",
      "775-th epoch val loss 1.0950789512915404\n",
      "776-th epoch train loss 1.1194594691145061\n",
      "776-th epoch val loss 1.0950791783642775\n",
      "777-th epoch train loss 1.1194596456083328\n",
      "777-th epoch val loss 1.0950794018859928\n",
      "778-th epoch train loss 1.1194598193423624\n",
      "778-th epoch val loss 1.095079621912197\n",
      "779-th epoch train loss 1.1194599903597278\n",
      "779-th epoch val loss 1.095079838497533\n",
      "780-th epoch train loss 1.119460158702889\n",
      "780-th epoch val loss 1.0950800516957908\n",
      "781-th epoch train loss 1.1194603244136438\n",
      "781-th epoch val loss 1.095080261559919\n",
      "782-th epoch train loss 1.1194604875331362\n",
      "782-th epoch val loss 1.095080468142041\n",
      "783-th epoch train loss 1.1194606481018683\n",
      "783-th epoch val loss 1.0950806714934633\n",
      "784-th epoch train loss 1.11946080615971\n",
      "784-th epoch val loss 1.0950808716646938\n",
      "785-th epoch train loss 1.1194609617459077\n",
      "785-th epoch val loss 1.0950810687054489\n",
      "786-th epoch train loss 1.1194611148990956\n",
      "786-th epoch val loss 1.0950812626646704\n",
      "787-th epoch train loss 1.1194612656573042\n",
      "787-th epoch val loss 1.095081453590533\n",
      "788-th epoch train loss 1.1194614140579697\n",
      "788-th epoch val loss 1.0950816415304612\n",
      "789-th epoch train loss 1.1194615601379445\n",
      "789-th epoch val loss 1.0950818265311373\n",
      "790-th epoch train loss 1.1194617039335049\n",
      "790-th epoch val loss 1.0950820086385145\n",
      "791-th epoch train loss 1.1194618454803607\n",
      "791-th epoch val loss 1.0950821878978274\n",
      "792-th epoch train loss 1.1194619848136629\n",
      "792-th epoch val loss 1.0950823643536047\n",
      "793-th epoch train loss 1.1194621219680163\n",
      "793-th epoch val loss 1.0950825380496796\n",
      "794-th epoch train loss 1.1194622569774817\n",
      "794-th epoch val loss 1.0950827090291988\n",
      "795-th epoch train loss 1.11946238987559\n",
      "795-th epoch val loss 1.095082877334637\n",
      "796-th epoch train loss 1.119462520695348\n",
      "796-th epoch val loss 1.0950830430078027\n",
      "797-th epoch train loss 1.1194626494692457\n",
      "797-th epoch val loss 1.0950832060898537\n",
      "798-th epoch train loss 1.1194627762292668\n",
      "798-th epoch val loss 1.0950833666213022\n",
      "799-th epoch train loss 1.1194629010068955\n",
      "799-th epoch val loss 1.0950835246420296\n",
      "800-th epoch train loss 1.1194630238331233\n",
      "800-th epoch val loss 1.0950836801912918\n",
      "801-th epoch train loss 1.1194631447384578\n",
      "801-th epoch val loss 1.0950838333077333\n",
      "802-th epoch train loss 1.11946326375293\n",
      "802-th epoch val loss 1.0950839840293936\n",
      "803-th epoch train loss 1.119463380906102\n",
      "803-th epoch val loss 1.095084132393718\n",
      "804-th epoch train loss 1.1194634962270738\n",
      "804-th epoch val loss 1.0950842784375663\n",
      "805-th epoch train loss 1.11946360974449\n",
      "805-th epoch val loss 1.095084422197222\n",
      "806-th epoch train loss 1.119463721486549\n",
      "806-th epoch val loss 1.0950845637084026\n",
      "807-th epoch train loss 1.1194638314810081\n",
      "807-th epoch val loss 1.095084703006267\n",
      "808-th epoch train loss 1.1194639397551907\n",
      "808-th epoch val loss 1.0950848401254236\n",
      "809-th epoch train loss 1.1194640463359928\n",
      "809-th epoch val loss 1.0950849750999416\n",
      "810-th epoch train loss 1.119464151249891\n",
      "810-th epoch val loss 1.0950851079633563\n",
      "811-th epoch train loss 1.1194642545229478\n",
      "811-th epoch val loss 1.095085238748679\n",
      "812-th epoch train loss 1.1194643561808186\n",
      "812-th epoch val loss 1.0950853674884062\n",
      "813-th epoch train loss 1.1194644562487577\n",
      "813-th epoch val loss 1.0950854942145254\n",
      "814-th epoch train loss 1.1194645547516247\n",
      "814-th epoch val loss 1.0950856189585239\n",
      "815-th epoch train loss 1.1194646517138906\n",
      "815-th epoch val loss 1.0950857417513975\n",
      "816-th epoch train loss 1.119464747159644\n",
      "816-th epoch val loss 1.0950858626236573\n",
      "817-th epoch train loss 1.1194648411125985\n",
      "817-th epoch val loss 1.0950859816053378\n",
      "818-th epoch train loss 1.1194649335960947\n",
      "818-th epoch val loss 1.095086098726004\n",
      "819-th epoch train loss 1.1194650246331097\n",
      "819-th epoch val loss 1.0950862140147573\n",
      "820-th epoch train loss 1.119465114246262\n",
      "820-th epoch val loss 1.095086327500246\n",
      "821-th epoch train loss 1.1194652024578153\n",
      "821-th epoch val loss 1.0950864392106698\n",
      "822-th epoch train loss 1.1194652892896866\n",
      "822-th epoch val loss 1.0950865491737882\n",
      "823-th epoch train loss 1.1194653747634493\n",
      "823-th epoch val loss 1.095086657416926\n",
      "824-th epoch train loss 1.1194654589003403\n",
      "824-th epoch val loss 1.0950867639669806\n",
      "825-th epoch train loss 1.1194655417212647\n",
      "825-th epoch val loss 1.0950868688504303\n",
      "826-th epoch train loss 1.1194656232468003\n",
      "826-th epoch val loss 1.0950869720933378\n",
      "827-th epoch train loss 1.119465703497203\n",
      "827-th epoch val loss 1.0950870737213598\n",
      "828-th epoch train loss 1.1194657824924135\n",
      "828-th epoch val loss 1.09508717375975\n",
      "829-th epoch train loss 1.1194658602520597\n",
      "829-th epoch val loss 1.0950872722333707\n",
      "830-th epoch train loss 1.1194659367954625\n",
      "830-th epoch val loss 1.0950873691666922\n",
      "831-th epoch train loss 1.1194660121416415\n",
      "831-th epoch val loss 1.0950874645838031\n",
      "832-th epoch train loss 1.1194660863093187\n",
      "832-th epoch val loss 1.0950875585084159\n",
      "833-th epoch train loss 1.119466159316923\n",
      "833-th epoch val loss 1.0950876509638723\n",
      "834-th epoch train loss 1.119466231182597\n",
      "834-th epoch val loss 1.0950877419731495\n",
      "835-th epoch train loss 1.1194663019241968\n",
      "835-th epoch val loss 1.0950878315588644\n",
      "836-th epoch train loss 1.1194663715593016\n",
      "836-th epoch val loss 1.0950879197432808\n",
      "837-th epoch train loss 1.1194664401052157\n",
      "837-th epoch val loss 1.0950880065483146\n",
      "838-th epoch train loss 1.1194665075789718\n",
      "838-th epoch val loss 1.0950880919955388\n",
      "839-th epoch train loss 1.1194665739973373\n",
      "839-th epoch val loss 1.0950881761061892\n",
      "840-th epoch train loss 1.1194666393768165\n",
      "840-th epoch val loss 1.0950882589011701\n",
      "841-th epoch train loss 1.1194667037336563\n",
      "841-th epoch val loss 1.0950883404010572\n",
      "842-th epoch train loss 1.1194667670838503\n",
      "842-th epoch val loss 1.0950884206261067\n",
      "843-th epoch train loss 1.1194668294431405\n",
      "843-th epoch val loss 1.0950884995962569\n",
      "844-th epoch train loss 1.119466890827024\n",
      "844-th epoch val loss 1.0950885773311343\n",
      "845-th epoch train loss 1.1194669512507551\n",
      "845-th epoch val loss 1.095088653850059\n",
      "846-th epoch train loss 1.1194670107293498\n",
      "846-th epoch val loss 1.0950887291720486\n",
      "847-th epoch train loss 1.1194670692775897\n",
      "847-th epoch val loss 1.0950888033158235\n",
      "848-th epoch train loss 1.1194671269100243\n",
      "848-th epoch val loss 1.0950888762998117\n",
      "849-th epoch train loss 1.1194671836409766\n",
      "849-th epoch val loss 1.0950889481421526\n",
      "850-th epoch train loss 1.1194672394845453\n",
      "850-th epoch val loss 1.0950890188607016\n",
      "851-th epoch train loss 1.1194672944546087\n",
      "851-th epoch val loss 1.0950890884730369\n",
      "852-th epoch train loss 1.119467348564828\n",
      "852-th epoch val loss 1.0950891569964591\n",
      "853-th epoch train loss 1.1194674018286515\n",
      "853-th epoch val loss 1.0950892244479997\n",
      "854-th epoch train loss 1.119467454259316\n",
      "854-th epoch val loss 1.0950892908444236\n",
      "855-th epoch train loss 1.1194675058698513\n",
      "855-th epoch val loss 1.0950893562022341\n",
      "856-th epoch train loss 1.1194675566730856\n",
      "856-th epoch val loss 1.095089420537676\n",
      "857-th epoch train loss 1.119467606681644\n",
      "857-th epoch val loss 1.0950894838667398\n",
      "858-th epoch train loss 1.1194676559079562\n",
      "858-th epoch val loss 1.0950895462051657\n",
      "859-th epoch train loss 1.1194677043642558\n",
      "859-th epoch val loss 1.0950896075684495\n",
      "860-th epoch train loss 1.1194677520625866\n",
      "860-th epoch val loss 1.0950896679718427\n",
      "861-th epoch train loss 1.1194677990148032\n",
      "861-th epoch val loss 1.0950897274303588\n",
      "862-th epoch train loss 1.1194678452325748\n",
      "862-th epoch val loss 1.0950897859587774\n",
      "863-th epoch train loss 1.1194678907273887\n",
      "863-th epoch val loss 1.0950898435716458\n",
      "864-th epoch train loss 1.1194679355105523\n",
      "864-th epoch val loss 1.095089900283285\n",
      "865-th epoch train loss 1.119467979593196\n",
      "865-th epoch val loss 1.095089956107791\n",
      "866-th epoch train loss 1.1194680229862766\n",
      "866-th epoch val loss 1.0950900110590396\n",
      "867-th epoch train loss 1.1194680657005789\n",
      "867-th epoch val loss 1.0950900651506899\n",
      "868-th epoch train loss 1.1194681077467201\n",
      "868-th epoch val loss 1.095090118396188\n",
      "869-th epoch train loss 1.1194681491351501\n",
      "869-th epoch val loss 1.095090170808768\n",
      "870-th epoch train loss 1.119468189876156\n",
      "870-th epoch val loss 1.0950902224014583\n",
      "871-th epoch train loss 1.1194682299798648\n",
      "871-th epoch val loss 1.0950902731870835\n",
      "872-th epoch train loss 1.1194682694562437\n",
      "872-th epoch val loss 1.0950903231782667\n",
      "873-th epoch train loss 1.1194683083151051\n",
      "873-th epoch val loss 1.095090372387436\n",
      "874-th epoch train loss 1.119468346566108\n",
      "874-th epoch val loss 1.0950904208268217\n",
      "875-th epoch train loss 1.1194683842187592\n",
      "875-th epoch val loss 1.095090468508465\n",
      "876-th epoch train loss 1.1194684212824175\n",
      "876-th epoch val loss 1.0950905154442188\n",
      "877-th epoch train loss 1.1194684577662968\n",
      "877-th epoch val loss 1.0950905616457505\n",
      "878-th epoch train loss 1.1194684936794639\n",
      "878-th epoch val loss 1.0950906071245434\n",
      "879-th epoch train loss 1.1194685290308464\n",
      "879-th epoch val loss 1.0950906518919041\n",
      "880-th epoch train loss 1.1194685638292312\n",
      "880-th epoch val loss 1.0950906959589597\n",
      "881-th epoch train loss 1.119468598083268\n",
      "881-th epoch val loss 1.095090739336665\n",
      "882-th epoch train loss 1.1194686318014713\n",
      "882-th epoch val loss 1.0950907820358022\n",
      "883-th epoch train loss 1.119468664992222\n",
      "883-th epoch val loss 1.0950908240669859\n",
      "884-th epoch train loss 1.11946869766377\n",
      "884-th epoch val loss 1.0950908654406641\n",
      "885-th epoch train loss 1.1194687298242372\n",
      "885-th epoch val loss 1.0950909061671223\n",
      "886-th epoch train loss 1.1194687614816174\n",
      "886-th epoch val loss 1.0950909462564833\n",
      "887-th epoch train loss 1.1194687926437794\n",
      "887-th epoch val loss 1.0950909857187139\n",
      "888-th epoch train loss 1.1194688233184702\n",
      "888-th epoch val loss 1.0950910245636232\n",
      "889-th epoch train loss 1.1194688535133135\n",
      "889-th epoch val loss 1.095091062800867\n",
      "890-th epoch train loss 1.1194688832358153\n",
      "890-th epoch val loss 1.0950911004399515\n",
      "891-th epoch train loss 1.1194689124933643\n",
      "891-th epoch val loss 1.0950911374902328\n",
      "892-th epoch train loss 1.1194689412932335\n",
      "892-th epoch val loss 1.095091173960922\n",
      "893-th epoch train loss 1.1194689696425808\n",
      "893-th epoch val loss 1.095091209861084\n",
      "894-th epoch train loss 1.1194689975484542\n",
      "894-th epoch val loss 1.0950912451996442\n",
      "895-th epoch train loss 1.11946902501779\n",
      "895-th epoch val loss 1.0950912799853876\n",
      "896-th epoch train loss 1.1194690520574169\n",
      "896-th epoch val loss 1.0950913142269605\n",
      "897-th epoch train loss 1.119469078674056\n",
      "897-th epoch val loss 1.0950913479328763\n",
      "898-th epoch train loss 1.1194691048743242\n",
      "898-th epoch val loss 1.0950913811115137\n",
      "899-th epoch train loss 1.1194691306647344\n",
      "899-th epoch val loss 1.0950914137711207\n",
      "900-th epoch train loss 1.119469156051697\n",
      "900-th epoch val loss 1.095091445919816\n",
      "901-th epoch train loss 1.1194691810415238\n",
      "901-th epoch val loss 1.0950914775655922\n",
      "902-th epoch train loss 1.1194692056404254\n",
      "902-th epoch val loss 1.0950915087163153\n",
      "903-th epoch train loss 1.1194692298545186\n",
      "903-th epoch val loss 1.0950915393797305\n",
      "904-th epoch train loss 1.1194692536898214\n",
      "904-th epoch val loss 1.0950915695634593\n",
      "905-th epoch train loss 1.1194692771522594\n",
      "905-th epoch val loss 1.095091599275007\n",
      "906-th epoch train loss 1.1194693002476654\n",
      "906-th epoch val loss 1.0950916285217587\n",
      "907-th epoch train loss 1.1194693229817798\n",
      "907-th epoch val loss 1.0950916573109846\n",
      "908-th epoch train loss 1.1194693453602544\n",
      "908-th epoch val loss 1.0950916856498425\n",
      "909-th epoch train loss 1.119469367388653\n",
      "909-th epoch val loss 1.0950917135453777\n",
      "910-th epoch train loss 1.1194693890724516\n",
      "910-th epoch val loss 1.095091741004525\n",
      "911-th epoch train loss 1.1194694104170397\n",
      "911-th epoch val loss 1.0950917680341103\n",
      "912-th epoch train loss 1.1194694314277243\n",
      "912-th epoch val loss 1.0950917946408534\n",
      "913-th epoch train loss 1.1194694521097277\n",
      "913-th epoch val loss 1.0950918208313685\n",
      "914-th epoch train loss 1.1194694724681917\n",
      "914-th epoch val loss 1.0950918466121677\n",
      "915-th epoch train loss 1.1194694925081774\n",
      "915-th epoch val loss 1.095091871989659\n",
      "916-th epoch train loss 1.1194695122346667\n",
      "916-th epoch val loss 1.0950918969701513\n",
      "917-th epoch train loss 1.1194695316525636\n",
      "917-th epoch val loss 1.095091921559856\n",
      "918-th epoch train loss 1.119469550766695\n",
      "918-th epoch val loss 1.0950919457648856\n",
      "919-th epoch train loss 1.1194695695818124\n",
      "919-th epoch val loss 1.095091969591257\n",
      "920-th epoch train loss 1.1194695881025944\n",
      "920-th epoch val loss 1.0950919930448952\n",
      "921-th epoch train loss 1.1194696063336442\n",
      "921-th epoch val loss 1.0950920161316295\n",
      "922-th epoch train loss 1.119469624279494\n",
      "922-th epoch val loss 1.0950920388571992\n",
      "923-th epoch train loss 1.1194696419446055\n",
      "923-th epoch val loss 1.0950920612272554\n",
      "924-th epoch train loss 1.1194696593333702\n",
      "924-th epoch val loss 1.0950920832473587\n",
      "925-th epoch train loss 1.1194696764501104\n",
      "925-th epoch val loss 1.0950921049229831\n",
      "926-th epoch train loss 1.1194696932990822\n",
      "926-th epoch val loss 1.095092126259518\n",
      "927-th epoch train loss 1.1194697098844741\n",
      "927-th epoch val loss 1.0950921472622677\n",
      "928-th epoch train loss 1.119469726210409\n",
      "928-th epoch val loss 1.0950921679364534\n",
      "929-th epoch train loss 1.119469742280946\n",
      "929-th epoch val loss 1.0950921882872158\n",
      "930-th epoch train loss 1.1194697581000799\n",
      "930-th epoch val loss 1.095092208319613\n",
      "931-th epoch train loss 1.1194697736717436\n",
      "931-th epoch val loss 1.0950922280386262\n",
      "932-th epoch train loss 1.119469788999808\n",
      "932-th epoch val loss 1.0950922474491578\n",
      "933-th epoch train loss 1.1194698040880842\n",
      "933-th epoch val loss 1.0950922665560334\n",
      "934-th epoch train loss 1.119469818940323\n",
      "934-th epoch val loss 1.0950922853640028\n",
      "935-th epoch train loss 1.1194698335602167\n",
      "935-th epoch val loss 1.0950923038777423\n",
      "936-th epoch train loss 1.1194698479514003\n",
      "936-th epoch val loss 1.095092322101854\n",
      "937-th epoch train loss 1.1194698621174506\n",
      "937-th epoch val loss 1.0950923400408696\n",
      "938-th epoch train loss 1.11946987606189\n",
      "938-th epoch val loss 1.0950923576992482\n",
      "939-th epoch train loss 1.1194698897881856\n",
      "939-th epoch val loss 1.0950923750813808\n",
      "940-th epoch train loss 1.1194699032997495\n",
      "940-th epoch val loss 1.0950923921915883\n",
      "941-th epoch train loss 1.1194699165999407\n",
      "941-th epoch val loss 1.095092409034124\n",
      "942-th epoch train loss 1.1194699296920658\n",
      "942-th epoch val loss 1.0950924256131767\n",
      "943-th epoch train loss 1.1194699425793797\n",
      "943-th epoch val loss 1.0950924419328667\n",
      "944-th epoch train loss 1.1194699552650864\n",
      "944-th epoch val loss 1.0950924579972516\n",
      "945-th epoch train loss 1.1194699677523396\n",
      "945-th epoch val loss 1.0950924738103263\n",
      "946-th epoch train loss 1.1194699800442436\n",
      "946-th epoch val loss 1.0950924893760212\n",
      "947-th epoch train loss 1.1194699921438547\n",
      "947-th epoch val loss 1.0950925046982067\n",
      "948-th epoch train loss 1.1194700040541803\n",
      "948-th epoch val loss 1.0950925197806916\n",
      "949-th epoch train loss 1.1194700157781825\n",
      "949-th epoch val loss 1.0950925346272264\n",
      "950-th epoch train loss 1.1194700273187757\n",
      "950-th epoch val loss 1.0950925492415018\n",
      "951-th epoch train loss 1.1194700386788279\n",
      "951-th epoch val loss 1.095092563627151\n",
      "952-th epoch train loss 1.1194700498611652\n",
      "952-th epoch val loss 1.0950925777877512\n",
      "953-th epoch train loss 1.1194700608685668\n",
      "953-th epoch val loss 1.0950925917268224\n",
      "954-th epoch train loss 1.1194700717037684\n",
      "954-th epoch val loss 1.0950926054478298\n",
      "955-th epoch train loss 1.1194700823694657\n",
      "955-th epoch val loss 1.0950926189541856\n",
      "956-th epoch train loss 1.1194700928683088\n",
      "956-th epoch val loss 1.095092632249247\n",
      "957-th epoch train loss 1.1194701032029084\n",
      "957-th epoch val loss 1.09509264533632\n",
      "958-th epoch train loss 1.1194701133758338\n",
      "958-th epoch val loss 1.0950926582186578\n",
      "959-th epoch train loss 1.1194701233896143\n",
      "959-th epoch val loss 1.095092670899464\n",
      "960-th epoch train loss 1.1194701332467398\n",
      "960-th epoch val loss 1.0950926833818906\n",
      "961-th epoch train loss 1.1194701429496599\n",
      "961-th epoch val loss 1.0950926956690408\n",
      "962-th epoch train loss 1.1194701525007882\n",
      "962-th epoch val loss 1.0950927077639703\n",
      "963-th epoch train loss 1.119470161902498\n",
      "963-th epoch val loss 1.0950927196696854\n",
      "964-th epoch train loss 1.119470171157128\n",
      "964-th epoch val loss 1.0950927313891465\n",
      "965-th epoch train loss 1.119470180266978\n",
      "965-th epoch val loss 1.0950927429252673\n",
      "966-th epoch train loss 1.1194701892343133\n",
      "966-th epoch val loss 1.095092754280916\n",
      "967-th epoch train loss 1.1194701980613635\n",
      "967-th epoch val loss 1.0950927654589158\n",
      "968-th epoch train loss 1.119470206750323\n",
      "968-th epoch val loss 1.0950927764620453\n",
      "969-th epoch train loss 1.1194702153033524\n",
      "969-th epoch val loss 1.0950927872930405\n",
      "970-th epoch train loss 1.1194702237225773\n",
      "970-th epoch val loss 1.0950927979545948\n",
      "971-th epoch train loss 1.1194702320100915\n",
      "971-th epoch val loss 1.095092808449358\n",
      "972-th epoch train loss 1.1194702401679557\n",
      "972-th epoch val loss 1.0950928187799402\n",
      "973-th epoch train loss 1.119470248198198\n",
      "973-th epoch val loss 1.0950928289489092\n",
      "974-th epoch train loss 1.1194702561028143\n",
      "974-th epoch val loss 1.0950928389587933\n",
      "975-th epoch train loss 1.1194702638837701\n",
      "975-th epoch val loss 1.0950928488120817\n",
      "976-th epoch train loss 1.119470271543\n",
      "976-th epoch val loss 1.0950928585112236\n",
      "977-th epoch train loss 1.1194702790824087\n",
      "977-th epoch val loss 1.095092868058631\n",
      "978-th epoch train loss 1.11947028650387\n",
      "978-th epoch val loss 1.0950928774566768\n",
      "979-th epoch train loss 1.1194702938092291\n",
      "979-th epoch val loss 1.0950928867076983\n",
      "980-th epoch train loss 1.1194703010003022\n",
      "980-th epoch val loss 1.0950928958139952\n",
      "981-th epoch train loss 1.1194703080788782\n",
      "981-th epoch val loss 1.095092904777832\n",
      "982-th epoch train loss 1.1194703150467158\n",
      "982-th epoch val loss 1.0950929136014367\n",
      "983-th epoch train loss 1.1194703219055482\n",
      "983-th epoch val loss 1.0950929222870036\n",
      "984-th epoch train loss 1.1194703286570793\n",
      "984-th epoch val loss 1.0950929308366917\n",
      "985-th epoch train loss 1.1194703353029891\n",
      "985-th epoch val loss 1.0950929392526272\n",
      "986-th epoch train loss 1.1194703418449292\n",
      "986-th epoch val loss 1.0950929475369018\n",
      "987-th epoch train loss 1.119470348284527\n",
      "987-th epoch val loss 1.0950929556915765\n",
      "988-th epoch train loss 1.1194703546233822\n",
      "988-th epoch val loss 1.0950929637186768\n",
      "989-th epoch train loss 1.119470360863072\n",
      "989-th epoch val loss 1.0950929716202007\n",
      "990-th epoch train loss 1.1194703670051465\n",
      "990-th epoch val loss 1.0950929793981106\n",
      "991-th epoch train loss 1.119470373051134\n",
      "991-th epoch val loss 1.0950929870543418\n",
      "992-th epoch train loss 1.1194703790025369\n",
      "992-th epoch val loss 1.0950929945907975\n",
      "993-th epoch train loss 1.1194703848608356\n",
      "993-th epoch val loss 1.095093002009351\n",
      "994-th epoch train loss 1.1194703906274854\n",
      "994-th epoch val loss 1.0950930093118472\n",
      "995-th epoch train loss 1.1194703963039212\n",
      "995-th epoch val loss 1.0950930165001012\n",
      "996-th epoch train loss 1.1194704018915538\n",
      "996-th epoch val loss 1.095093023575901\n",
      "997-th epoch train loss 1.1194704073917725\n",
      "997-th epoch val loss 1.095093030541005\n",
      "998-th epoch train loss 1.1194704128059452\n",
      "998-th epoch val loss 1.0950930373971464\n",
      "999-th epoch train loss 1.1194704181354171\n",
      "999-th epoch val loss 1.0950930441460278\n"
     ]
    }
   ],
   "source": [
    "slr = ScratchLinearRegression(num_iter=1000, lr=0.01, no_bias=True, verbose=True)\n",
    "slr.fit(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3d1c185-546e-46f9-924d-fb9aca0d4c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x15e434b48c0>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(slr.loss)\n",
    "plt.plot(slr.val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2860188-2d7a-4bf7-94ed-c6a2e5f25840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y_pred, y):\n",
    "\n",
    "    mse = np.mean((y-y_pred)**2)\n",
    "    \n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8451301-fbde-4f59-96fc-e2997e290158",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = slr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22e5b72e-0af1-442d-a2b6-293b24cd4b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83805433724.15932"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE(np.exp(pred_test), np.exp(y_test)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
